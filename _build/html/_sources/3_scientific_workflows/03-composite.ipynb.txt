{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f370075-64f3-4784-aca4-cd560e188786",
   "metadata": {},
   "source": [
    "# Compositing Rasters with Dask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f33cad-bcae-474f-86dc-13fd601d41d7",
   "metadata": {},
   "source": [
    "An integral part of a geospatial developers workflow will occassionally require composite rasters to represent specific temporal ranges as a single image. This approach is often performed to visualize cloud-free imagery over an AOI. \n",
    "\n",
    "In the following example, we will create a cloud-free median composite. Using \"median\" as the compositing method is, one, simple, and two, provides adequate resiliency against cloud shadow/edges that are not necessarily caputured by the cloud-mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9fd8d-8556-4a3d-b8bc-4f7f173baa26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be using Sentinel-2 L2A imagery from Microsoft Planetary Computer STAC server:\n",
    "!pip install planetary_computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718eead-3c3c-4a11-9971-a9143e850dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import pystac\n",
    "import stackstac\n",
    "import datetime\n",
    "import planetary_computer\n",
    "import dask\n",
    "import json\n",
    "import gcsfs\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from dask_gateway import Gateway\n",
    "# from shapely.geometry import Point\n",
    "from pystac_client import Client\n",
    "from dask.distributed import performance_report\n",
    "from typing import List\n",
    "from rio_cogeo.cogeo import cog_translate\n",
    "from rio_cogeo.profiles import cog_profiles\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720e7a7-d192-4c97-8c6c-ac53594cc468",
   "metadata": {},
   "source": [
    "Set up the Dask Cluser and GCSFS Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198f018d-3c53-4432-b24c-03fd57d55673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_gcsfs_client(username:str):\n",
    "    # set up the gcsfs system with credentials\n",
    "    print('registering gcsfs')\n",
    "    tok = os.path.join(os.environ['HOME'], f'geoanalytics_{username}', 'geo.json') # Change this to your own cred file\n",
    "    tok_dict = json.load(open(tok)) # cloudpickle will just send the path to tok, which the processing nodes can't see. Send json instead\n",
    "    gcs = gcsfs.GCSFileSystem(token=tok_dict, access='read_write')\n",
    "    return gcs\n",
    "\n",
    "def register_dask_client(imgname:str=None):\n",
    "    ''' Make the gcsfs filesystem available with credentials and start client '''\n",
    "    # we want to set up a cluster\n",
    "    client = None\n",
    "    cluster = None\n",
    "    print('registering cluster')\n",
    "    gateway = Gateway()\n",
    "    options = gateway.cluster_options()\n",
    "    if not imgname is None:\n",
    "        options.image = imgname\n",
    "    cluster = gateway.new_cluster(options)\n",
    "    print(cluster.name)\n",
    "    client = cluster.get_client()\n",
    "    client.restart() # flush nodes\n",
    "    return client, cluster, options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d6030-d352-4acc-8285-49b2311d91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = input('Username: ')\n",
    "gcs = register_gcsfs_client(username=username)\n",
    "client, cluster, options = register_dask_client(imgname='pangeo/pangeo-notebook:2022.04.15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4992d-f237-46b6-9e59-a05a556664c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Dask cluster details\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d25f3-6830-4115-a3df-090511bb95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Dask cluster to 30 workers\n",
    "cluster.scale(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefb923-fd42-4e36-9b5d-de3a1a599580",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47feaf48-cd3d-4a13-b539-303b671f5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write from the dask cluster to the remote bucket\n",
    "@dask.delayed\n",
    "def write_ras(gcs, epsg, ras, b, pth):\n",
    "    import rioxarray\n",
    "    try:\n",
    "        ds = xr.Dataset()\n",
    "        ras = ras.rio.write_crs(epsg)\n",
    "        ras.rio.to_raster('ras.tif')\n",
    "        # Turn the raster into a COG\n",
    "        dst_profile = cog_profiles.get(\"deflate\")\n",
    "            cog_translate(\n",
    "                'ras.tif',\n",
    "                'ras_cog.tif',\n",
    "                dst_profile,\n",
    "                in_memory=True,\n",
    "                quiet=False,\n",
    "            )\n",
    "        # Use GCSFS Client to put COG into remote bucket\n",
    "        gcs.put('ras_cog.tif', pth)\n",
    "        # Clean up rasters on Dask Worker\n",
    "        os.remove('ras.tif')\n",
    "        os.remove('ras_cog.tif')\n",
    "        return 'success'\n",
    "    except Exception as e:\n",
    "        # Return error and associated band\n",
    "        return f'{b}: {e}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98a440-ca4a-4055-977a-ad96bc5793db",
   "metadata": {},
   "source": [
    "## AOI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5fcd4-a70a-4919-9a62-435772ec9917",
   "metadata": {},
   "source": [
    "This AOI was generated from: https://www.keene.edu/campus/maps/tool/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fee941-91a7-4cd1-8ad2-0d90ee1666c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Rough Polygon around Vancouver in EPSG4326\n",
    "poly = {\n",
    "  \"coordinates\": [\n",
    "    [\n",
    "      [\n",
    "        -122.1185303,\n",
    "        49.3304921\n",
    "      ],\n",
    "      [\n",
    "        -123.3078003,\n",
    "        49.3644891\n",
    "      ],\n",
    "      [\n",
    "        -123.2528687,\n",
    "        48.7742927\n",
    "      ],\n",
    "      [\n",
    "        -122.1817017,\n",
    "        48.7579995\n",
    "      ],\n",
    "      [\n",
    "        -122.1185303,\n",
    "        49.3304921\n",
    "      ]\n",
    "    ]\n",
    "  ],\n",
    "  \"type\": \"Polygon\"\n",
    "}\n",
    "poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57f68e-2828-43c9-9181-d6320d644365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll write this to a file, so geopanadas can open it \n",
    "poly_file_pth = '/tmp/geo.geojson'\n",
    "with open(poly_file_pth, 'w') as f:\n",
    "   json.dump(poly, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7abb9-421e-4a7f-9fee-69517fa24b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AOI from local filesystem\n",
    "f = gpd.read_file(poly_file_pth)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716c268-e8ce-4f44-999d-d6bd5bffcca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FOOTPRINT needs to be enveloped for pystac_client to query with\n",
    "# More complex shapes can be be clipped with at later stages of the workflow\n",
    "FOOTPRINT = f.to_crs('epsg:4326').geometry[0].envelope\n",
    "FOOTPRINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fe879-8821-4ba8-87a3-274613fc2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOOTPRINT.bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ba1a9-a533-4b77-914e-50f28362fe89",
   "metadata": {},
   "source": [
    "## Set Up STAC Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba4d5c-0cf9-43fc-bcbf-2a995fb7194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up STAC client\n",
    "api = Client.open('https://planetarycomputer.microsoft.com/api/stac/v1')\n",
    "api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb793a-610a-47bc-b396-971bffe40e21",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea675fa2-1f3e-401d-9077-6188ea78d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG \n",
    "# -------------\n",
    "BASE_PTH = 'gs://geoanalytics-user-shared-data'\n",
    "OUTPUT_DIR = 'tutorial_test'\n",
    "TGT_BANDS =  ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B11', 'B12', 'B8A']\n",
    "YEARS = ['2020']\n",
    "BEGIN_MONTH = '06'\n",
    "END_MONTH = '09'\n",
    "MAX_CLOUD = 20.0\n",
    "READ_IN_CHUNK = 4096\n",
    "RESOLUTION = 10\n",
    "TEMPORAL_CHUNK = {'time': -1, 'band': 1, 'x': 128, 'y': 128}\n",
    "SYNCHRONOUS = False # Write bands out one at a time - use if resources can't handle all bands at once for AOI\n",
    "# -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e75c2e-4f47-4e5e-a9d4-594abdd1ad38",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703577c-b061-4fd6-a07e-99f1e70c1d0c",
   "metadata": {},
   "source": [
    "In the main loop we iterate over the number of target years, creating a composite of each for each of the composite bands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876996d-bc27-4c0d-8a52-9bd3cf4cb6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Main pipeline to iterate over years\n",
    "write_futs = []\n",
    "for year in YEARS:\n",
    "    OUT_PTH = f'{BASE_PTH}/{OUTPUT_DIR}/{year}'\n",
    "    date_range = f'{year}-{BEGIN_MONTH}-01/{year}-{END_MONTH}-30'\n",
    "    \n",
    "    # Query the Planetary Computer STAC server with pystac_client\n",
    "    print(f'[Querying] {year}')\n",
    "    items = api.search(\n",
    "        collections = ['sentinel-2-l2a'],\n",
    "        intersects = FOOTPRINT,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": MAX_CLOUD}},\n",
    "        datetime = date_range,\n",
    "    ).get_all_items()\n",
    "    \n",
    "    print(f'\\tFound {len(items)} items')\n",
    "    # planetarycomputer requires signed URLs to access Asset HREFs. \n",
    "    print('\\t[Signing data links]')\n",
    "    signed_items = [planetary_computer.sign(item).to_dict() for item in items]\n",
    "    \n",
    "    # Pull out SCL DataArray before bands are looped through\n",
    "    # since this will not change per band. \n",
    "    scl_stk = (\n",
    "        stackstac.stack(\n",
    "            signed_items,\n",
    "            assets=['SCL'], \n",
    "            chunksize=READ_IN_CHUNK, # Set chunksize\n",
    "            resolution=RESOLUTION, # Set all bands res to this\n",
    "            bounds_latlon=FOOTPRINT.bounds, # clip to AOI bounds\n",
    "        )\n",
    "    )\n",
    "    # Create binary mask [np.nan, 1]\n",
    "    # https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm\n",
    "    scl_stk.data = da.where(\n",
    "            ((scl_stk.data==0)| # nodata\n",
    "             (scl_stk.data==1)| # Saturated or Defective\n",
    "             (scl_stk.data==8)| # cloud: medium probability\n",
    "             (scl_stk.data==9)| # cloud: high probability\n",
    "             (scl_stk.data==10)| # cloud: thin cirrus\n",
    "             (scl_stk.data==11) # snow\n",
    "            ), np.nan, 1)\n",
    "    \n",
    "    # Iterate over bands and build composite DAG\n",
    "    for band in TGT_BANDS:\n",
    "        clear_output(wait=True) # clear Jupyter Cell output\n",
    "        print(f'[Processing {band}]')\n",
    "\n",
    "        # Convert STAC query into a xarray.DataArray\n",
    "        # with stackstac\n",
    "        print('\\t[Converting STAC query to DataArray]')\n",
    "        data = (\n",
    "            stackstac.stack(\n",
    "                signed_items,\n",
    "                assets=[band], \n",
    "                chunksize=READ_IN_CHUNK, # Set chunksize\n",
    "                resolution=RESOLUTION, # Set all bands res to this\n",
    "                bounds_latlon=FOOTPRINT.bounds, # clip to AOI bounds\n",
    "            ).where(lambda x: x > 0, other=np.nan) # Convert nodata zero to np.nan\n",
    "        )\n",
    "        \n",
    "        # Mask the bands with the accompanying SCL band per time\n",
    "        print('\\t[Masking data with SCL]')\n",
    "        masked = data.copy()\n",
    "        masked.data = data.data * scl_stk.data # np.nan will mask unwated pixels\n",
    "        \n",
    "        # Create median composite\n",
    "        print('\\t[Creating Median composite]')\n",
    "        # skip np.nan in temporal stack with skipna=True\n",
    "        median = masked.median(dim='time', skipna=True, keep_attrs=True)\n",
    "        median = median.chunk({'band': 1, 'y': 'auto', 'x': 'auto'})\n",
    "        median = median.transpose('band', 'y', 'x')\n",
    "    \n",
    "        # Cast the xarray.DataArray to int16\n",
    "        median = median.astype(np.uint16)\n",
    "\n",
    "        # Get EPSG from median metadata\n",
    "        epsg = median.coords['epsg'].values.tolist()\n",
    "\n",
    "        \n",
    "        if SYNCHRONOUS:\n",
    "            # Issues with large AOI's - limited resources - so compute each composite\n",
    "            # individually to relieve the Dask Cluster\n",
    "            print(dask.compute(write_ras(gcs, epsg, median, band, f'{OUT_PTH}/{band}.tif')))\n",
    "        else:\n",
    "            # Write out each band to a file asynchronously\n",
    "            print(f'\\t[Processing and Writing {band}]')\n",
    "            median.name = band\n",
    "            median.attrs['long_name'] = band\n",
    "            write_futs.append(write_ras(gcs, epsg, median, band, f'{OUT_PTH}/{band}.tif'))\n",
    "\n",
    "if not SYNCHRONOUS:\n",
    "    clear_output(wait=True)\n",
    "    write_futs.visualize()\n",
    "    with performance_report('dask_report.html'):\n",
    "        print(dask.compute(write_futs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dfb59a-e821-4a55-bf4b-9e3a4203a694",
   "metadata": {},
   "source": [
    "## Shut Dask Cluster Down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ef50f-bc49-49dd-ab4c-a981f7c2da89",
   "metadata": {},
   "source": [
    "Make sure to shut down the Dask Cluster as not to incur costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e354a39-69af-492e-b710-281f01cdd6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e6e9d-e33a-4d72-86c3-07ae4c9f4ce9",
   "metadata": {},
   "source": [
    "## Remove Intermediate Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b83d6d-19b1-4207-952a-cbed1f40ad0e",
   "metadata": {},
   "source": [
    "The files in /tmp will be deleted at shutdown of your session, however, if you plan on continuing to work, then cleaning up old and no-longer-needed files is helpful housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153eef48-d999-467e-8205-a33cca6a0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(poly_file_pth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
