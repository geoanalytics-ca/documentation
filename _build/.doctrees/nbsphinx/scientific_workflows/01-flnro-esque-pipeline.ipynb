{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLNRO-esque Kubeflow Pipeline for Daily NDSI Averages \n",
    "FLNRO-esque demonstrates the workflow for gathering data products that are specific to an application and how to leverage those into an informative product that can affect real-world decisions, making use of powerful tools such as Kubeflow.\n",
    "\n",
    "In this tutorial notebook, we will build a Kubeflow Pipeline which will process the Modis Snow Product over the previous 5 to 8 days (depending on user input) and return the visualization of the NDSI Average in British Columbia, Canada. \n",
    "\n",
    "The main steps for creating the Daily NDSI Average Pipeline:\n",
    "\n",
    "1. Query MODIS snow product (MOD10A1.001) data using a bounding box AOI and date ranges\n",
    "2. Threshold the NDSI\n",
    "3. Create mosaics from raw MODIS granules by merging different tiles together\n",
    "5. Compute the NDSI average over the date range\n",
    "6. Visualize the NDSI averages\n",
    "\n",
    "All these steps for this tutorial will be separated into pipeline components, based on Python functions we will implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**First, run the following command to install all the packages and dependencies required for this tutorial**. The requirements.txt file contains the Python CMR API for data querying, and the Kubeflow Pipelines SDK and lxml (a Python library which allows for easy handling of XML and HTML files, and can also be used for web scraping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r data/requirements_6.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import io\n",
    "import yaml\n",
    "import json\n",
    "import logging\n",
    "import getpass\n",
    "import requests\n",
    "\n",
    "import kfp\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "from lxml import html\n",
    "from google.cloud import storage\n",
    "from kubernetes import client\n",
    "from kubernetes.client import V1Toleration\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func\n",
    "from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Kubeflow Client\n",
    "\n",
    "To create a pipeline, we must first connect to GEOAnalytics' Kubeflow Pipeline Client. Please run the cell below and enter your GEOAnalytics username and password to authenticate the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GEOAnalyticsKubeflowClient\n",
    "\n",
    "class GEOAnalyticsKubeflowClient:\n",
    "    def __init__(self, username, password, auth_provider, namespace=None):\n",
    "        self.logger = None\n",
    "        self._initialize_logger()\n",
    "\n",
    "        self.client = None\n",
    "        self.namespace = username if namespace is None else namespace\n",
    "        \n",
    "        self._authenticate(username, password, auth_provider)\n",
    "        self._validate_client_connection()\n",
    "\n",
    "    def _initialize_logger(self):\n",
    "        logging.basicConfig(format='%(asctime)s %(levelname)s (%(name)s): %(message)s', level=logging.INFO, datefmt='%Y-%m-%d %H:%M:%S')\n",
    "        self.logger = logging.getLogger(\"GEOAnalyticsKubeflowClient\")\n",
    "\n",
    "    # https://github.com/kubeflow/kfctl/issues/140#issuecomment-719894529\n",
    "    def _authenticate(self, username, password, auth_provider):\n",
    "        session = requests.Session()\n",
    "        response = session.get(auth_provider.host)\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        }\n",
    "\n",
    "        data = auth_provider.get_auth_data_dict(username, password)\n",
    "        post_url = auth_provider.get_auth_post_url(response)\n",
    "        session.post(post_url, headers=headers, data=data)\n",
    "        try:\n",
    "            session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "        except Exception as e:\n",
    "            message = \"invalid host or credentials\"\n",
    "            self.logger.error(message)\n",
    "            raise Exception(message) from None\n",
    "\n",
    "        self.client = kfp.Client(\n",
    "            host=f\"{auth_provider.host}/pipeline\",\n",
    "            cookies=f\"authservice_session={session_cookie}\",\n",
    "            namespace=self.namespace,\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _validate_client_connection(self):\n",
    "        if self.client.list_pipelines().total_size > 0:\n",
    "            self.logger.info(\"successfully authenticated with kubeflow\")\n",
    "        else:\n",
    "            message = \"unable to validate kubeflow client connection. listing pipelines failed.\"\n",
    "            self.logger.error(message)\n",
    "            raise Exception(message)\n",
    "            \n",
    "class DexProvider:\n",
    "    def __init__(self, host):\n",
    "        self.host = host\n",
    "        self.name = \"dex\"\n",
    "\n",
    "    def get_auth_data_dict(self, username, password):\n",
    "        return {\"login\": username, \"password\": password}\n",
    "\n",
    "    def get_auth_post_url(self, initial_response):\n",
    "        return initial_response.url\n",
    "\n",
    "    def get_provider_name(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  asaini\n",
      " ············\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-08 19:35:27 INFO (GEOAnalyticsKubeflowClient): successfully authenticated with kubeflow\n"
     ]
    }
   ],
   "source": [
    "provider = DexProvider(\"http://kubeflow.geoanalytics.ca\")\n",
    "kubeflow_client = GEOAnalyticsKubeflowClient(input(\"Username: \"), getpass.getpass(), provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we can move forward to building our functions, make sure you have an **existing NASA EarthData account**. We require authentication for retrieving the MODIS data in this example. If you do not have an EarthData account yet, you can create it for free here: https://urs.earthdata.nasa.gov/home\n",
    "\n",
    "Then, enter in your **username** and **password** in the cell input below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " asaini\n",
      " ·············\n"
     ]
    }
   ],
   "source": [
    "username = input()\n",
    "password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing the Python Functions  \n",
    "Once we have our authentications ready, let's move on and create the functions. The python functions will implement the  steps for the working pipeline, which will be converted to components using the `kfp.components.create_component_from_func(<python function>, <base image>)`. The custom Docker image, stored at *'registry.geoanalytics.ca/examples/modis-ndsi'*, is already created with all the necessary packages and files installed, ready for you to use. \n",
    "\n",
    "### 2.1 Querying Data Function\n",
    "\n",
    "The first step of the process to to collect information about the data we want. Given the number of most recent days and an area of interest, the function must output a list of granules URLs.\n",
    "\n",
    "The function:\n",
    "\n",
    "- Creates a EarthData client passing in the username and password collected in above, using Hatfield CMR.\n",
    "- Opens the polygon shapefile (saved in our Docker image) and uses its bounds as the area of interest.\n",
    "- Creates a date range based on the days provided.\n",
    "- Queries for granules matching the provided requirements. \n",
    "- Separate URLs of granules into sublists according to their dates.\n",
    "- Return the list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_modis_data(days: int, polygon: str, earthdata_user: str, earthdata_pass: str) -> list:\n",
    "    import datetime\n",
    "    import json\n",
    "    import re\n",
    "    import glob\n",
    "    import geopandas as gpd\n",
    "    from hatfieldcmr import CMRClient\n",
    "    \n",
    "    # Create earthdata client\n",
    "    ed_client = CMRClient(None, earthdata_user, earthdata_pass)\n",
    "    \n",
    "     # Query for granule metadata from NASA CMR\n",
    "    bc = gpd.read_file(glob.glob(polygon + '/*.shp')[0])\n",
    "    bounding_box = bc.bounds.values[0]\n",
    "    # End date is 4 days ago from current date to allow enough time for correctly processed data\n",
    "    end_date = datetime.date.today() - datetime.timedelta(4) \n",
    "    start_date = end_date - datetime.timedelta(days-1)\n",
    "    product = 'MOD10A1.6'\n",
    "\n",
    "    granules = ed_client.query(str(start_date), str(end_date), product, bbox=bounding_box)\n",
    "    print(f\"Retrieved {len(granules)} granules from query\")\n",
    "\n",
    "    # Separate urls of granules into sublists according to dates \n",
    "    urls_list = []\n",
    "    sublist = []\n",
    "    \n",
    "    get_date = re.search('\\d{4}\\d{3}', granules[0]['links'][0]['href'])\n",
    "    prev_date = datetime.datetime.strptime(get_date.group(), '%Y%j').date()\n",
    "\n",
    "    for file in granules:\n",
    "        url = file['links'][0]['href']\n",
    "        get_date = re.search('\\d{4}\\d{3}', url)\n",
    "        new_date = datetime.datetime.strptime(get_date.group(), '%Y%j').date()\n",
    "        \n",
    "        # Condition to separate to a different sublist: if date is different or its the last granule in the list\n",
    "        if new_date != prev_date or file == granules[-1]: \n",
    "            sublist_json = json.dumps(sublist) \n",
    "            urls_list.append(sublist_json)\n",
    "            prev_date = new_date\n",
    "            sublist =[]\n",
    "            continue\n",
    "\n",
    "        sublist.append(url)\n",
    "    \n",
    "    # Serializing json  \n",
    "    urls_list_json = json.dumps(urls_list) \n",
    "    return urls_list_json\n",
    "\n",
    "\n",
    "query_modis_data_op = create_component_from_func(query_modis_data,base_image='registry.geoanalytics.ca/examples/modis-ndsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Thresholding and Creating Mosaic Function\n",
    "The function `create_mosaics()` combines the two steps of thresholding the NDSI of each granule, and combining same date granules together.\n",
    "\n",
    "The inputs of this function are the sublist of urls of the same date, EarthData credentials, the polygon of British Columbia, and the path to our GEOAnalytics bucket where our generated mosaics will stored.\n",
    "\n",
    "We will be using the MOD10A Version 6 Product which included the computed NDSI Snow Cover values. The NDSI formula is the normalized difference between highly Visible (VIR) and Shortwave Infrared (SWIR) bands. We will need to threshold to make sure to separate the snow-covered pixels from no-snow pixels. Pixels with values between 20 and 100 are considered **Snow Pixels**.\n",
    "\n",
    "The function:\n",
    "\n",
    "- Sets up the Google Cloud Storage File System (GCSFS) API's credentials for accessing our buckets. \n",
    "    - Token is the secret key JSON file which is loaded into the Docker container.\n",
    "- Create an EarthData Client.\n",
    "- Iterates through each URL in the list of URLS (of the same date)\n",
    "    - Using the Hatfield CMR, stream in the granule into a temporary file.\n",
    "    - Threshold the NDSI (Snow pixels set to 1, otherwise 0).              \n",
    "    - Append the updated DataArray to a list.\n",
    "- Merge the list of DataArrays together.\n",
    "- Reproject the merged array to the same Coordinate Reference System (CRS) as the BC Polygon.\n",
    "- Clip the mosaic to the polygon.\n",
    "- Write the mosaic raster to a local file.\n",
    "- Copy the contents from the local file to a folder for the current date on Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mosaics(urls: list, earthdata_user: str, earthdata_pass: str, polygon: str, gcs_paths: str) -> OutputPath(str):\n",
    "    import datetime\n",
    "    import re\n",
    "    import glob\n",
    "    import tempfile\n",
    "    import rasterio\n",
    "    import gcsfs\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "    import rioxarray as rxr\n",
    "    import xarray as xr\n",
    "    from hatfieldcmr import CMRClient\n",
    "    from urllib.request import urlopen, Request, build_opener, HTTPCookieProcessor\n",
    "    from rioxarray.merge import merge_arrays\n",
    "    \n",
    "    # Set up the gcsfs system with credentials\n",
    "    tok = '/secret/gcp-credentials/geoanalytics-canada-kubeflow.json'\n",
    "    tok_dict = json.load(open(tok)) \n",
    "    gcs = gcsfs.GCSFileSystem(token=tok_dict, access='read_write')\n",
    "\n",
    "    # Create earthdata client\n",
    "    ed_client = CMRClient(None, earthdata_user, earthdata_pass)\n",
    "    \n",
    "    today = datetime.date.today()\n",
    "    downloads = []\n",
    "    for url in urls:        \n",
    "        # Get date for file naming\n",
    "        get_date = re.search('\\d{4}\\d{3}', url)\n",
    "        date = datetime.datetime.strptime(get_date.group(), '%Y%j').date()\n",
    "\n",
    "        # Stream in the current granule \n",
    "        buff = ed_client.download_earthdata_file(url, (earthdata_user, earthdata_pass)) # download the image into memory\n",
    "        with tempfile.NamedTemporaryFile() as tmpfile:\n",
    "            tmpfile.write(buff.getvalue())\n",
    "            with rxr.open_rasterio(tmpfile.name, 'r') as src:\n",
    "                ndsi = src.NDSI_Snow_Cover\n",
    "                # Threshold NDSI\n",
    "                ndsi.data[(ndsi.data > 100)] = 0\n",
    "                ndsi.data[(ndsi.data < 20)] = 0\n",
    "                ndsi.data[(ndsi.data != 0)] = 1\n",
    "                ndsi.data = ndsi.data.astype(np.uint8)\n",
    "\n",
    "        downloads.append(ndsi)\n",
    "    \n",
    "    #Merge granules together into a mosaic\n",
    "    ds = merge_arrays(downloads)\n",
    "    print('Dataset size (Gb): ', ds.nbytes/1e9)\n",
    "    \n",
    "    # Reproject and crop to the polygon\n",
    "    bc = gpd.read_file(glob.glob(polygon + '/*.shp')[0])\n",
    "    bounding_box = bc.bounds.values[0]\n",
    "    ds = ds.rio.reproject(bc.crs)\n",
    "    ds = ds.rio.clip_box(*bounding_box)\n",
    "    print('Dataset size (Gb): ', ds.nbytes/1e9)\n",
    "\n",
    "    file = f'{date}.tif'\n",
    "    ds.rio.to_raster(file, recalc_transform=True)\n",
    "\n",
    "    # Saving to GCP\n",
    "    output_path = f'{gcs_paths}{today}/mosaics/{date}.tif'\n",
    "    gcs.put(file, output_path)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "create_mosaics_op = create_component_from_func(create_mosaics, base_image='registry.geoanalytics.ca/examples/modis-ndsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Computing Average NDSI Function\n",
    "\n",
    "Once the mosaics for all the dates are created, the next step will require a function which stacks these mosaics and calculates the average NDSI value of the mosaics. The GCS path will be passed in as input, pointing to where all the mosaics are stored. \n",
    "\n",
    "The function:\n",
    "\n",
    "- Sets up the GCSFS credentials for accessing the bucket (same as in previous function)\n",
    "- Iteratively search for and save the dimensions and shape of the mosaic with the smallest size.\n",
    "    - All mosaic arrays must be same size to be stacked together.\n",
    "- Resize all mosaics to the saved shape. \n",
    "- Make sure the number of mosaics equals to the number of days that are requested.\n",
    "- Stack the mosaics together.\n",
    "- Compute the mean over all the mosaics.\n",
    "- Place the averaged mosaics in a DataArray with the saved dimensions.\n",
    "- Write the mosaic raster to a local file.\n",
    "- Copy the contents from the local file into the same location where the mosaics are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_ndsi(gcs_paths: str, days: int) -> str:\n",
    "    import datetime\n",
    "    import glob\n",
    "    import json\n",
    "    import gcsfs\n",
    "    import rasterio\n",
    "    import numpy as np\n",
    "    import rioxarray as rxr\n",
    "    import xarray as xr\n",
    "    \n",
    "     # Set up the gcsfs system with credentials\n",
    "    tok = '/secret/gcp-credentials/geoanalytics-canada-kubeflow.json'\n",
    "    tok_dict = json.load(open(tok)) \n",
    "    gcs = gcsfs.GCSFileSystem(token=tok_dict, access='read_write')\n",
    "    \n",
    "    today = datetime.date.today()\n",
    "    mosaics = gcs.glob(f'{gcs_paths}{today}/mosaics/')\n",
    "    \n",
    "    # Arbitrary variables to be replaced\n",
    "    _dims_x = None\n",
    "    _dims_y = None\n",
    "    _shape = (1, 100000, 100000)\n",
    "    \n",
    "    # Get the shape of the smallest mosaic for stacking\n",
    "    for mosaic in mosaics:\n",
    "        if not(mosaic.endswith('.tif')):\n",
    "            continue  \n",
    "        with gcs.open(mosaic) as fobj:\n",
    "            with rasterio.open(fobj) as dataset:\n",
    "                ndsi = xr.open_rasterio(dataset)\n",
    "                if ndsi.shape < _shape:\n",
    "                    _shape = ndsi.shape\n",
    "                    _dims_x = ndsi['x']\n",
    "                    _dims_y = ndsi['y']\n",
    "    \n",
    "    # Resize all the mosaics to the size of the smallest one\n",
    "    arrs = []\n",
    "    for mosaic in mosaics:\n",
    "        if not(mosaic.endswith('.tif')):\n",
    "            continue  \n",
    "        with gcs.open(mosaic) as fobj:\n",
    "            with rasterio.open(fobj) as dataset:\n",
    "                ndsi = xr.open_rasterio(dataset)\n",
    "                ndsi.data[(ndsi.data > 100)] = 0\n",
    "                arrs.append(ndsi.data[:,0:_shape[1],0:_shape[2]])\n",
    "    # Assert that the number of mosaics matches the days requested\n",
    "    assert(len(arrs) == days)\n",
    "    \n",
    "    stk = np.stack(arrs, axis=0)  # stacking\n",
    "    stk = stk.squeeze()           # squeezing\n",
    "    stk_mn = np.mean(stk, axis=0) # averaging\n",
    "    \n",
    "    # Combining and returning the DataArray \n",
    "    ds = xr.DataArray(data=stk_mn, coords=(_dims_y,_dims_x))\n",
    "    \n",
    "    file = 'average_ndsi.tif'\n",
    "    ds.rio.to_raster(file, recalc_transform=True)\n",
    "\n",
    "    # Saving to GCP\n",
    "    output_path = f'{gcs_paths}{today}/{file}'\n",
    "    gcs.put(file, output_path)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "average_ndsi_op = create_component_from_func(average_ndsi, base_image='registry.geoanalytics.ca/examples/modis-ndsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Plotting the Final NDSI Function\n",
    "Once the mosaics for all the dates are created, the next step will require a function which stacks these mosaics and calculates the average NDSI value of the mosaics. The GCS path will be passed in as input, pointing to where all the mosaics are stored.\n",
    "\n",
    "The function:\n",
    "\n",
    "- Sets up the GCSFS credentials for accessing the bucket.\n",
    "- Load in the average NDSI tiff stored in GCS.\n",
    "- Crop to the BC Polygon shape.\n",
    "- Plot the image with the boundary of BC outlining the edges.\n",
    "- Save the figure as a temporary binary file.\n",
    "- Encode to base64 so it can be shown as in image source in HTML as a \"Pipeline Output Artifact.\"\n",
    "- Write the metadata for the output viewer and save as a metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ndsi(final_ndsi: str, polygon: str, days: int, mlpipeline_ui_metadata_path: OutputPath()):\n",
    "    import glob\n",
    "    import json\n",
    "    import gcsfs\n",
    "    import datetime\n",
    "    import rasterio\n",
    "    import base64\n",
    "    import rioxarray as rxr\n",
    "    import geopandas as gpd\n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from io import BytesIO\n",
    "    from shapely.geometry import mapping\n",
    "    \n",
    "     # Set up the gcsfs system with credentials\n",
    "    tok = '/secret/gcp-credentials/geoanalytics-canada-kubeflow.json'\n",
    "    tok_dict = json.load(open(tok)) \n",
    "    gcs = gcsfs.GCSFileSystem(token=tok_dict, access='read_write')\n",
    "    \n",
    "    # Load in the tiff file from GCS as a DataArray and crop to BC\n",
    "    with gcs.open(final_ndsi) as fobj:\n",
    "            with rasterio.open(fobj) as dataset:\n",
    "                ndsi = rxr.open_rasterio(dataset)\n",
    "                bc = gpd.read_file(glob.glob(polygon + '/*.shp')[0])\n",
    "                ndsi = ndsi.rio.write_crs(bc.crs)\n",
    "                ndsi = ndsi.rio.clip(bc.geometry.apply(mapping), bc.crs)\n",
    "    \n",
    "    # Plot the image\n",
    "    f, ax = plt.subplots(figsize=(16, 8))\n",
    "    ndsi[0].plot.imshow(ax=ax, cmap=plt.cm.RdYlBu, vmin=0, vmax=1, clim=[0,1])\n",
    "    bc.boundary.plot(ax=ax, alpha=.8, edgecolor=\"black\")#, facecolor=\"None\")\n",
    "    ax.set(title=f\"NDSI average over {days} days\")  \n",
    "    \n",
    "    # Save image as a binary temp file to be encoded to base64\n",
    "    tmpfile = BytesIO()\n",
    "    plt.savefig(tmpfile, format='png')\n",
    "    encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "\n",
    "    html = '<img src=\\'data:image/png;base64,{}\\'>'.format(encoded)\n",
    "        \n",
    "    metadata = {\n",
    "    'outputs' : [\n",
    "    {\n",
    "      'source': html,\n",
    "      'storage': 'in-line',\n",
    "      'type': 'markdown',\n",
    "    }]}\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "    \n",
    "plot_ndsi_op = create_component_from_func(plot_ndsi, base_image='registry.geoanalytics.ca/examples/modis-ndsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pipeline Output Artifact:*** The Kubeflow Pipelines UI provides built-in support for many types of visualizations. The current available *Output Viewers* are Confusion matrix, Markdown, ROC curve, Table, TensorBoard, and Web app. For our example, we want to output a final Raster image using matplotlib, so the output viewers we can use are Markdown or **Web app.** These will allow us to pass in static HTML code that can be displayed in the Output Artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Delete Files Function\n",
    "Since our pipeline stores the files we've created for today and this is a Daily NDSI Average Pipeline, it is unnecessary to keep the data once we have the final result. The function `delete_files()` will permanently remove the folder for today's date containing all the data stored in the GEOAnalytic's Google Cloud bucket.\n",
    "\n",
    "The function:\n",
    "\n",
    "- Sets up the GCSFS credentials for accessing the bucket.\n",
    "- Access the folder for today's date within the GCS bucket.\n",
    "- Check if files exist within this folder.\n",
    "- Check if the folder contains subfolders.\n",
    "    - Delete files from the subfolder.\n",
    "- Delete all files from the folder (This will implicitly delete the empty folder itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(gcs_paths: str, mlpipeline_ui_metadata_path: OutputPath()):\n",
    "    import json\n",
    "    import gcsfs\n",
    "    import glob\n",
    "    import datetime\n",
    "    \n",
    "     # Set up the gcsfs system with credentials\n",
    "    tok = '/secret/gcp-credentials/geoanalytics-canada-kubeflow.json'\n",
    "    tok_dict = json.load(open(tok)) \n",
    "    gcs = gcsfs.GCSFileSystem(token=tok_dict, access='read_write')\n",
    "    \n",
    "    # Access the directory with files created for today's run\n",
    "    today = datetime.date.today()\n",
    "    \n",
    "    main_folder = gcs_paths + str(today) \n",
    "    cond = gcs.exists(main_folder)\n",
    "    if cond:\n",
    "        folder = gcs.ls(main_folder)\n",
    "        for file in folder:\n",
    "            subfolder = gcs.ls(file)\n",
    "            if len(subfolder) > 1:  # If the object is a subfolder\n",
    "                empty_subfile = [gcs.rm(x) for x in subfolder] # Delete all files in subfolder\n",
    "            gcs.rm(file) # Delete other files in the main folder\n",
    "\n",
    "delete_files_op = create_component_from_func(delete_files, base_image='registry.geoanalytics.ca/examples/modis-ndsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assembling the Pipeline\n",
    "Once we have our components ready, the next step is to put them together in a pipeline. The Daily NDSI Pipeline will take the arguments for the number of days you want an average over, your EarthData username and password, the path to the polygon shapefile (this file is already downloaded in the base image), and the GCS path where files will be stored. \n",
    "\n",
    "Now there are two important variables to notice: \n",
    "\n",
    " - `custom_nodepool` which is an operator that configures the Google Kubernetes Engine (GKE) preemptible in a container op, custom to this tutorial. \n",
    " - `gcp_key`, an operator to configure the container to use Google Cloud Platform (GCP) service account by service account key stored in a Kubernetes secret. This allows us to read and write to GEOAnalytics GCS buckets.\n",
    " \n",
    "Applying these operators to the components will allow the pipeline to run smoothly. We will also set the CPI request at 3.5 and memory request at 11,000 MB to enforce operations running on individual pods.\n",
    "\n",
    "1. The first component to be called in the converted `query_modis_data` function. \n",
    "\n",
    "2. The next step is to threshold the NDSI and combine the rasters for the same date in a mosaic using `create_mosaic`. To efficiently do this, we will use the `dsl.ParallelFor` loop which will run the process operation on different pods, where each pod will take sublist of the list of granule URLs as input. i.e. Each pod will process granules of a different date, so the number of pods should be equivalent to the number of days passed in. \n",
    "\n",
    "3. After we have the mosaics created, we will compute the average of the NDSI with our `average_ndsi_op` operation. the `.after(processOp)` method constrains the operation to start once the previous parallel operations are complete.\n",
    "\n",
    "4. Finally, passing in the output path from the previous operation to the averaged GeoTIFF, we can plot the final image. \n",
    "\n",
    "5. An optional step is to delete the files created and stored within the GCS. If you want to take a look at the saved files, the mosaics and the final average NDSI (all GeoTIFFs), then simply comment out the `deleteOp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='MOD10A1 Daily NDSI Average Processing Pipeline',\n",
    "  description='A pipeline that processes the daily Normalized Difference Snow Index in British Columbia given a time range.'\n",
    ")\n",
    "def daily_ndsi_pipeline(days: int, earthdata_user: str, earthdata_pass: str, polygon: str, gcs_paths: str): \n",
    "    \n",
    "    custom_nodepool = gcp.use_preemptible_nodepool(V1Toleration(key='daily-ndsi-processing', operator='Exists'))\n",
    "    gcp_key = gcp.use_gcp_secret(secret_name='user-gcp-sa', secret_file_path_in_volume='/geoanalytics-canada-kubeflow.json')\n",
    "    \n",
    "    # 1. Fetch the URLs of our MODIS data from the query\n",
    "    fetchOp = query_modis_data_op(days, \n",
    "                                  polygon, \n",
    "                                  earthdata_user, \n",
    "                                  earthdata_pass\n",
    "                                 ).apply(custom_nodepool)#.set_cpu_request('3.5').set_memory_request('11000M') \n",
    "    \n",
    "    # 2. \"Download\" the data and create mosaics stored in a list\n",
    "    with dsl.ParallelFor(fetchOp.output) as item: \n",
    "        processOp = create_mosaics_op(item, \n",
    "                                      earthdata_user, \n",
    "                                      earthdata_pass, \n",
    "                                      polygon,\n",
    "                                      gcs_paths\n",
    "                                     ).apply(custom_nodepool).apply(gcp_key).set_cpu_request('3.5').set_memory_request('11000M').set_retry(3)\n",
    "    \n",
    "    # 3. Get average of the all the mosaics\n",
    "    combineOp = average_ndsi_op(gcs_paths,\n",
    "                               days).after(processOp).apply(custom_nodepool).apply(gcp_key)#.set_cpu_request('3.5').set_memory_request('11000M').set_retry(3)\n",
    "    \n",
    "    # 4. Plot the average (input is an Xarray DataArray)\n",
    "    plottingOp = plot_ndsi_op(combineOp.output, \n",
    "                              polygon, \n",
    "                              days).apply(custom_nodepool).apply(gcp_key)#.set_cpu_request('3.5').set_memory_request('11000M').set_retry(3)\n",
    "    \n",
    "     # 5. Delete files from GCS\n",
    "    deleteOp = delete_files_op(gcs_paths).after(plottingOp).apply(custom_nodepool).apply(gcp_key)#.set_cpu_request('3.5').set_memory_request('11000M').set_retry(3)\n",
    "    \n",
    "    # Configurations for the pipeline to run in the specific nodepool and allowing access to GEOAnalytics' container Rrgistry\n",
    "    dsl.get_pipeline_conf().set_default_pod_node_selector('app.geoanalytics.ca/nodepool', 'daily-ndsi-processing')\n",
    "    dsl.get_pipeline_conf().set_image_pull_secrets([client.V1LocalObjectReference(name=\"regcredgeoanalytics\")])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile the pipeline with the name **\"Average NDSI Pipeline!\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE PIPELINE #\n",
    "pipeline_name = \"Average NDSI Pipeline!\"\n",
    "pipeline_func = daily_ndsi_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.yaml'\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments we will pass in below are to be used for the Run. The username and password will be what you entered above in Section 1, and you can modify the days parameter to any reasonable integer. The polygon and gcs_paths should stay the same as they are hardcoded for this tutorial example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    'days': 8,\n",
    "    'earthdata_user': username,\n",
    "    'earthdata_pass': password,\n",
    "    'polygon': '/bc-polygon',\n",
    "    'gcs_paths': 'gcs://geoanalytics-user-shared-data/tutorial_data/'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run and upload the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=1 # only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-08 19:36:44 INFO (root): Creating experiment daily-avg-ndsi.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://kubeflow.geoanalytics.ca/pipeline/#/experiments/details/23d4aee2-ce95-4869-8a7e-5e309180afc4\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://kubeflow.geoanalytics.ca/pipeline/#/runs/details/21f8d435-b8ae-46ae-a4a3-eda8af64251d\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If the experiment under the given name already exists - get it's ID, else - create a new experiment\n",
    "try:\n",
    "    experiment_id = kubeflow_client.client.get_experiment(\"daily-avg-ndsi\")\n",
    "except:\n",
    "    experiment = kubeflow_client.client.create_experiment(\"daily-avg-ndsi\")\n",
    "run_name = pipeline_func.__name__ + f'-run#{count}'\n",
    "run_result = kubeflow_client.client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)\n",
    "count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=http://kubeflow.geoanalytics.ca/pipeline/#/pipelines/details/0d40765c-4a91-4ba0-af2e-1fde412d00c0>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "echo_pipeline = kubeflow_client.client.upload_pipeline(pipeline_filename, pipeline_name, \"DAILY AVERAGE NDSI PIPELINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Results\n",
    "\n",
    "Head over to the Kubeflow Pipeline UI and take a look at the Run you created. Once finished, the pipeline graph should look like this image below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/scientific_workflows_images/01_pipeline-graph.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the final plot of the Averaged NDSI over the past few days, click on the `plot_ndsi` operation and under the **\"Input/Output\"**, click on the link in the Output Artifacts section. This is open a new tab with the final plot shown, which should look similar to this: \n",
    "\n",
    "<img src=\"../images/scientific_workflows_images/01_pipeline-result.png\">\n",
    "\n",
    "***Note: This NDSI was processed on September 2nd, 2021.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional Sources\n",
    "\n",
    "For additional information take a look at these resources:\n",
    "\n",
    "- Information about the MOD10A Product: https://nsidc.org/data/MOD10A1/versions/6\n",
    "- Building Python Function-based Components: https://www.kubeflow.org/docs/components/pipelines/sdk/v2/python-function-components/\n",
    "- The GCSFS Documentation: https://fs-gcsfs.readthedocs.io/en/latest/\n",
    "- Information about Kubeflow Pipelines Output Artifacts: https://www.kubeflow.org/docs/components/pipelines/sdk/output-viewer/\n",
    "- The Kubeflow Pipeline SDK API Documentation: https://kubeflow-pipelines.readthedocs.io/en/latest/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoanalytics",
   "language": "python",
   "name": "geoanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
