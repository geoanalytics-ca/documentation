{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e803afb-edd2-49e6-8ee1-985f1055a501",
   "metadata": {},
   "source": [
    "# MODIS Monthly Snow Preprocessing Pipeline & ARD Analytics Using Dask\n",
    "\n",
    "The following documentation leverages GEOAnalytics Canada's JupyterHub, Container Registry, Pipeline, and Dask Cluster systems to collect, preprocess, and then analyse data.\n",
    "\n",
    "Gathering and preprocessing data can be easily automated using GEOAnalytics' Pipeline system. \n",
    "This particular Workflow is tailored toward [MODIS Snow Cover Daily (modis-10A1-061) product, a Level-3 product produced by NASA](https://nsidc.org/sites/default/files/mod10a1-v061-userguide_1.pdf) data. \n",
    "Each Task in the Pipeline system uses a custom Docker Image from [GEOAnalytics Container Registry](https://docs.geoanalytics.ca/1_getting_started/08-container-registry.html) (CR). \n",
    "> **Note**\n",
    "> GEOAnalytics CR portal for members of Earth Obervation for Public Health is found at: https://registry.eo4ph.geoanalytics.ca\n",
    "\n",
    "Quoting the User Guide:\n",
    ">Snow-covered land typically has very high reflectance in visible bands and very low reflectance in\n",
    "shortwave infrared bands. The Normalized Difference Snow Index (NDSI) reveals the magnitude of\n",
    "this difference. The snow cover algorithm calculates NDSI for all land and inland water pixels in\n",
    "daylight using Terra MODIS band 4 (visible green) and band 6 (shortwave near-infrared).\n",
    "\n",
    "We will use AOI's defined by [Canada's Open Data Provincial Administrative Boudnaries](https://open.canada.ca/data/en/dataset/306e5004-534b-4110-9feb-58e3a5c3fd97/resource/15ea678b-8667-43ce-b80c-6545e039ee00) to drill down on a Province's snow cover. \n",
    "Using this AOI to determine data intersections, we preprocess accordingly to a minimal ARD output, preserving all original data values, saving out to Cloud Storage to be used in subsequent analytics. \n",
    "\n",
    "GEOAnalytics Canada Pipelines leverages Argo Workflows as the underlying workflow manager and we recommend using Python Hera framework to declare the components, as shown below.\n",
    "\n",
    "The Pipeline portion of this code relies on the following core package versions:\n",
    "```bash\n",
    "hera==5.5.1\n",
    "xarray==2023.7.0\n",
    "rioxarray==0.15.0\n",
    "pystac_client==0.7.2\n",
    "planetary-computer==1.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e16fa5-0b65-47f6-811b-16b746b670d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:21.123418Z",
     "iopub.status.busy": "2023-08-29T23:53:21.122623Z",
     "iopub.status.idle": "2023-08-29T23:53:21.130848Z",
     "shell.execute_reply": "2023-08-29T23:53:21.130207Z",
     "shell.execute_reply.started": "2023-08-29T23:53:21.123391Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global Configuration\n",
    "# --------------------\n",
    "registry_url = 'registry.eo4ph.geoanalytics.ca'\n",
    "repository = 'tutorial'\n",
    "image_version_tag = '0.1.1'\n",
    "docker_image_tag = f'{registry_url}/{repository}/snow-cover:{image_version_tag}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f0a8e-08f5-4152-b166-7130c10c96a8",
   "metadata": {},
   "source": [
    "## Create Custom Container Image From Dockerfile\n",
    "\n",
    "To be able to execute the methods we will be implementing below, we will create a simple and custom minimal environment necessary for our functions to run in.\n",
    "In this documentation, we use Python's Docker framework to build our Image, log in to [GEOAnalytics Container Registry](https://docs.geoanalytics.ca/1_getting_started/08-container-registry.html) (CR), and finally push our Image to be used in our Pipeline Workflow. \n",
    "The Image is built using the Docker in Docker (DinD) sidecar, accessible over localhost and preconfigured for the JupyterHub session to use. \n",
    "\n",
    "The Pipeline system is capable of pulling Images from GEOAnalytics CR.\n",
    "\n",
    "The same effect can be accomplished via an external Dockerfile and using the Docker CLI commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bacc60-dc4b-4915-b081-2abda35ae68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --user docker==6.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4aca27-c15b-42ef-9877-d126f1a3248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section Imports\n",
    "# ---------------\n",
    "import os\n",
    "import io\n",
    "import getpass\n",
    "import docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b267b-0641-44b5-ad32-44f6aedb7bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DinD (Docker in Docker) sidecar is accessible over localhost\n",
    "docker_client = docker.from_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed495cc-e1de-4fc2-802f-e4bf83b6c33b",
   "metadata": {},
   "source": [
    "The following code shows a String representation of a Dockerfile which can be turned into a Pythonic file-like object, which the Python Docker framework `build()` method accepts. \n",
    "Alternatively, if keeping such things in memory is not necessary, a file path on the filesystem can be used to point at a Dockerfile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2c92b-4e7a-45e8-875d-c3aa9b85f2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dockerfile = \"\"\"\n",
    "FROM python:3.11-slim\n",
    "RUN apt update --no-install-recommends \\\n",
    "    >>/dev/null\n",
    "    \n",
    "RUN pip install \\\n",
    "        pandas==2.0.3 \\\n",
    "        geopandas==0.13.2 \\\n",
    "        numpy==1.24.4 \\\n",
    "        pystac==1.8.3 \\\n",
    "        pystac_client==0.7.2 \\\n",
    "        planetary-computer==1.0.0 \\\n",
    "        odc-stac==0.3.6 \\\n",
    "        azure-storage-blob==12.17.0 \\\n",
    "        xarray==2023.7.0 \\\n",
    "        rioxarray==0.15.0 \\\n",
    "        pyproj==3.6.0 \\\n",
    "        shapely==2.0.1 \\\n",
    "        Pillow==10.0.0\n",
    "\"\"\"\n",
    "dockerfile_file = io.BytesIO(dockerfile.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c221d-7f59-4e0e-b1d8-df4dbe963ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_file.name = 'dockerfile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd99d4-7c35-433a-a59b-ada182a252f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_client.images.build(fileobj=dockerfile_file, tag=docker_image_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363478c-a6f1-45f0-b7e5-470415af05f3",
   "metadata": {},
   "source": [
    "Log in to GEOAnalytics CR using your username (inferred from environment variable) and password. \n",
    "You will need to log in in order to 1. verify that you are a authenticated User and 2. be able to push to your public and private repositories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c964f0-6332-49c1-a784-adacad7b253c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docker_client.login(\n",
    "    username=os.getenv('JUPYTERHUB_USER'),\n",
    "    password=getpass.getpass(),\n",
    "    registry=registry_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad781dc0-5a60-4b43-bf67-e405a9ad9afc",
   "metadata": {},
   "source": [
    "Finally, push your new custom Image to the CR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd3f06-deb4-4bfe-be08-7a3ad4668ac6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "docker_client.images.push(docker_image_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065cba02-d01f-44d2-89ee-1141d2266883",
   "metadata": {},
   "source": [
    "## Pipeline Workflow\n",
    "\n",
    "This section decalares the components used in the preprocessing workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98084b3f-7e41-49d3-bab5-eb0546069e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:27.833342Z",
     "iopub.status.busy": "2023-08-29T23:53:27.832577Z",
     "iopub.status.idle": "2023-08-29T23:53:28.556057Z",
     "shell.execute_reply": "2023-08-29T23:53:28.555313Z",
     "shell.execute_reply.started": "2023-08-29T23:53:27.833304Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "from hera.shared import global_config\n",
    "from hera.workflows import (\n",
    "                        Artifact, \n",
    "                        Workflow, \n",
    "                        script, \n",
    "                        DAG, \n",
    "                        Resources, \n",
    "                        Parameter, \n",
    "                        Env,\n",
    "                        RetryStrategy,\n",
    "                        models as m,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f9302-ef00-4653-994c-85614273412c",
   "metadata": {},
   "source": [
    "Set the Global configuration for Argo Workflows - the underlying system for GEOAnalytics Canada Pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22333b10-b27f-4525-b6f1-12473f263d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:30.377269Z",
     "iopub.status.busy": "2023-08-29T23:53:30.376275Z",
     "iopub.status.idle": "2023-08-29T23:53:30.383912Z",
     "shell.execute_reply": "2023-08-29T23:53:30.383117Z",
     "shell.execute_reply.started": "2023-08-29T23:53:30.377235Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_config.api_version = \"argoproj.io/v1\"\n",
    "global_config.host = os.getenv(\"WORKFLOW_HOST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531084e2-de9f-4bb4-bb6e-1723e7404903",
   "metadata": {},
   "source": [
    "GEOAnalytics Canada offers a variety of Node types to provision workloads on to. To determine what is available, contact the Admin group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d638a8-b054-4af2-90b9-aac1c968fbb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:31.697128Z",
     "iopub.status.busy": "2023-08-29T23:53:31.696688Z",
     "iopub.status.idle": "2023-08-29T23:53:32.093353Z",
     "shell.execute_reply": "2023-08-29T23:53:32.092608Z",
     "shell.execute_reply.started": "2023-08-29T23:53:31.697100Z"
    }
   },
   "outputs": [],
   "source": [
    "user_config = None\n",
    "with open(f'/home/jovyan/geoanalytics_{os.getenv(\"JUPYTERHUB_USER\")}/userConfig.yaml', 'r') as f:\n",
    "    user_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34308d26-2a5e-4585-a0db-abd7f6873c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:32.754930Z",
     "iopub.status.busy": "2023-08-29T23:53:32.754135Z",
     "iopub.status.idle": "2023-08-29T23:53:32.758205Z",
     "shell.execute_reply": "2023-08-29T23:53:32.757437Z",
     "shell.execute_reply.started": "2023-08-29T23:53:32.754904Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_node_config = user_config['pipeline']['node-config']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c7da7-ade6-4847-abc9-bcdf8adab9c4",
   "metadata": {},
   "source": [
    "The first tasks we need to implement are those to gather the prerequisite AOI artifact and to prepare the provided temporal range that will be parallelised in the next steps. \n",
    "\n",
    "The AOI file is hosted as a zipped up Shapefile. Since Canada's Data portal is Open, we can use Python's `requests` library to `GET` the data. \n",
    "However, Shapefiles are archaic and clumsy when not on a local filesystem - so we will convert this into GeoJSON file that is more Cloud-friendly. \n",
    "The GeoJSON file will be saved as an Artifact, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6872336d-d20f-495c-b297-511928a9ee2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T00:07:17.084826Z",
     "iopub.status.busy": "2023-08-30T00:07:17.084123Z",
     "iopub.status.idle": "2023-08-30T00:07:17.092154Z",
     "shell.execute_reply": "2023-08-30T00:07:17.091396Z",
     "shell.execute_reply.started": "2023-08-30T00:07:17.084801Z"
    }
   },
   "outputs": [],
   "source": [
    "@script(\n",
    "    image=docker_image_tag,\n",
    "    resources=Resources(cpu_request='100m', memory_request='512Mi'),\n",
    "    node_selector=pipeline_node_config['small']['nodeSelector'],\n",
    "    tolerations=pipeline_node_config['small']['tolerations'],\n",
    "    outputs=[Artifact(name='aoi', path='/tmp/aoi.geojson')]\n",
    ")\n",
    "def generate_polygon(\n",
    "        url: str,\n",
    "        target_province: str\n",
    "    ):\n",
    "    import os\n",
    "    import io\n",
    "    import zipfile\n",
    "    import requests\n",
    "\n",
    "    os.environ['USE_PYGEOS'] = '0'\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    from glob import glob\n",
    "    from shapely.geometry import Polygon, MultiPolygon\n",
    "    \n",
    "    try:\n",
    "        os.makedirs('/tmp/canvec')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    res = requests.get(url)\n",
    "    with zipfile.ZipFile(io.BytesIO(res.content), 'r') as zf:\n",
    "        zf.extractall(path='/tmp/canvec')\n",
    "\n",
    "    shp_file = glob('/tmp/canvec/**/*geo_pol*_2.shp')[0]\n",
    "    gdf = gpd.read_file(shp_file)\n",
    "\n",
    "    # Pull out Adminitrative Boundary to usable Shape\n",
    "    tgt_col = 'juri_en'\n",
    "    bc_all = gdf[gdf[tgt_col] == target_province]\n",
    "    mp = MultiPolygon([poly for poly in bc_all.geometry.values.tolist() if isinstance(poly, Polygon)])\n",
    "    bc = gpd.GeoDataFrame(data={'geom': [mp]})\n",
    "    bc = bc.set_geometry('geom')\n",
    "\n",
    "    # Populate an Artifact, defined in the wrapper, with the GeoJSON file\n",
    "    bc.to_file('/tmp/aoi.geojson', driver='GeoJSON')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d0709-9378-4e42-8d2e-895574afc051",
   "metadata": {},
   "source": [
    "Using the configuration set by the User at the main workflow's entrypoint, we generate a \"payload\" for subsequent Tasks which will allow the desired set of data to be gathered and processed. \n",
    "\n",
    "As a bonus - this Task can also be configured in a [CRON Workflow](https://argoproj.github.io/argo-workflows/cron-workflows/) (a Workflow that executes on a specific schedule) ([Hera implementation](https://hera-workflows.readthedocs.io/en/latest/examples/workflows/upstream/cron_workflow/?h=cron)) where the `years` and `months` arguments may be `None` and therefore default to the current year and month, as defined by the functions code.\n",
    "Then a CRON schedule for the beginning of the month (given a few days for L3 data to be processed and uploaded) can be set to ingest data of interest automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1b8174-f55a-4096-992d-672fe53e55a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:38.511975Z",
     "iopub.status.busy": "2023-08-29T23:53:38.511361Z",
     "iopub.status.idle": "2023-08-29T23:53:38.520205Z",
     "shell.execute_reply": "2023-08-29T23:53:38.519483Z",
     "shell.execute_reply.started": "2023-08-29T23:53:38.511948Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@script(\n",
    "    image=docker_image_tag,\n",
    "    resources=Resources(cpu_request='100m', memory_request='128Mi'),\n",
    "    node_selector=pipeline_node_config['small']['nodeSelector'],\n",
    "    tolerations=pipeline_node_config['small']['tolerations']\n",
    ")\n",
    "def fan_out_temporal_range(\n",
    "        catalog_url: str,\n",
    "        collection: str,\n",
    "        assets: List[str],\n",
    "        years: List[int] | None, # Will default to current year\n",
    "        months: List[int] | None, # Will default to current month \n",
    "        epsg: int,\n",
    "        resolution: int,\n",
    "        target_province: str,\n",
    "        output_prefix: str = '',\n",
    "    ):\n",
    "    import sys\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    if years is not None:\n",
    "        for year in years:\n",
    "            if year < 2000:\n",
    "                raise ValueError(f'MODIS collection starts at 02/24/2000: Error {start_year}')\n",
    "            elif year > int(datetime.now().strftime('%Y')):\n",
    "                raise ValueError(f'Cannot query future: Error {end_year}')\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        years = [int(datetime.now().strftime('%Y'))]\n",
    "\n",
    "    if months is not None:\n",
    "        for month in months:\n",
    "            if month < 1 or month > 12:\n",
    "                raise ValueError(f'Month range is from 1 to 12, inclusive: Error: {start_month}')\n",
    "    else:\n",
    "        months = [int(datetime.now().strftime('%m'))]\n",
    "        \n",
    "    temporal_range = []\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            if month < 10:\n",
    "                month = f'0{month}'\n",
    "\n",
    "            payload = {\n",
    "                'catalog_url': catalog_url,\n",
    "                'collection': collection,\n",
    "                'assets': assets,\n",
    "                'year': year,\n",
    "                'month': month,\n",
    "                'epsg': epsg,\n",
    "                'resolution': resolution,\n",
    "                'prov': target_province,\n",
    "                'output_prefix': output_prefix,\n",
    "            }\n",
    "            temporal_range.append(payload)\n",
    "    json.dump(temporal_range, sys.stdout)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77788436-6f39-4866-8ff1-87ec5f1531fc",
   "metadata": {},
   "source": [
    "Using the temporal range list and the AOI Artifact, we can parallelise querying for available data from [Planetary Computer's STAC server](https://planetarycomputer.microsoft.com/dataset/modis-10A1-061#overview).\n",
    "MODIS Snow Cover Daily, as the name suggests, is a daily product. \n",
    "However, as can be seen in the following Task, the dates gathered by the initial query are converted into a new \"payload\" for the next Task. \n",
    "This implements some resiliency to missing dates or when targetting a non-daily product in order to successfully gather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ad0a54e-1535-4e20-a074-6ab073a5928c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:39.229097Z",
     "iopub.status.busy": "2023-08-29T23:53:39.228370Z",
     "iopub.status.idle": "2023-08-29T23:53:39.236611Z",
     "shell.execute_reply": "2023-08-29T23:53:39.235914Z",
     "shell.execute_reply.started": "2023-08-29T23:53:39.229064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@script(\n",
    "    image=docker_image_tag,\n",
    "    resources=Resources(cpu_request='500m', memory_request=\"3Gi\"),\n",
    "    node_selector=pipeline_node_config['small']['nodeSelector'],\n",
    "    tolerations=pipeline_node_config['small']['tolerations'],\n",
    "    inputs=[Artifact(name='aoi', path='/tmp/aoi.geojson')]\n",
    ")\n",
    "def query_for_data(month_payload: Dict[str, Any]):\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import pystac_client\n",
    "    import planetary_computer\n",
    "\n",
    "    os.environ['USE_PYGEOS'] = '0'\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        os.makedirs('/tmp/payloads')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    payload = month_payload\n",
    "    \n",
    "    geom = gpd.read_file('/tmp/aoi.geojson', driver='GeoJSON').iloc[0].geometry\n",
    "\n",
    "    catalog = pystac_client.Client.open(\n",
    "        payload['catalog_url']\n",
    "    )\n",
    "\n",
    "    query_time_range = f\"{payload['year']}-{payload['month']}\"\n",
    "    query = catalog.search(\n",
    "        collections=payload['collection'],\n",
    "        intersects=geom.envelope,\n",
    "        datetime=query_time_range\n",
    "    )\n",
    "\n",
    "    date_track_list = []\n",
    "    item_payloads = []\n",
    "    for item in query.items_as_dicts():\n",
    "        target_date = datetime.strftime(\n",
    "            datetime.strptime(\n",
    "                item['properties']['start_datetime'], \n",
    "                '%Y-%m-%dT%H:%M:%SZ'\n",
    "            ), '%Y-%m-%d')\n",
    "        if target_date not in date_track_list:\n",
    "            date_track_list.append(target_date)\n",
    "            item_payload = {}\n",
    "            item_payload['query-date'] = target_date\n",
    "            item_payload.update(payload)\n",
    "            item_payloads.append(item_payload)\n",
    "    \n",
    "    json.dump(item_payloads, sys.stdout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a77e20-5a4c-4d10-a62f-d298ddcd024c",
   "metadata": {},
   "source": [
    "The following Task is the engine of the preprocessing - the data is pulled and operated on. \n",
    "As networks can be susceptible to down-time, a `RetryStrategy` is implemented in order to attempt again in case of a failed communication. \n",
    "If the request fails up to the limit, then this and subsequent Tasks will also fail. \n",
    "The AOI Artifact retrieved at the beginning of the Workflow is now used not just to query but also to clip the retrieved data. \n",
    "This minimises unwanted data being stored and further processed - avoiding wasting compute on what would be discarded otherwise. \n",
    "Each Task will receive a list of payloads targetting the temporal window defined in the previous step (a month in this specific case). \n",
    "Once preprocessed, the data is saved to Cloud storage and the path to that data is delivered to the next Task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb28049a-65ae-484d-8040-463815eb464c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:40.554411Z",
     "iopub.status.busy": "2023-08-29T23:53:40.554097Z",
     "iopub.status.idle": "2023-08-29T23:53:40.567230Z",
     "shell.execute_reply": "2023-08-29T23:53:40.566565Z",
     "shell.execute_reply.started": "2023-08-29T23:53:40.554386Z"
    }
   },
   "outputs": [],
   "source": [
    "@script(\n",
    "    image=docker_image_tag,\n",
    "    resources=Resources(cpu_request='2000m', memory_request=\"16Gi\"),\n",
    "    node_selector=pipeline_node_config['medium']['nodeSelector'],\n",
    "    tolerations=pipeline_node_config['medium']['tolerations'],\n",
    "    retry_strategy=RetryStrategy(\n",
    "            limit=10,\n",
    "            backoff=m.Backoff(\n",
    "                duration=\"1\",\n",
    "                factor=\"2\",\n",
    "                max_duration=\"1m\",\n",
    "            )\n",
    "    ),\n",
    "    inputs=[\n",
    "        Artifact(name='aoi', path='/tmp/aoi.geojson')\n",
    "    ],\n",
    ")\n",
    "def process_daily_granules(payloads: List[Any]):\n",
    "    import io\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import xarray\n",
    "    import pyproj\n",
    "    import odc.stac\n",
    "    import rioxarray\n",
    "    import pystac_client\n",
    "    import planetary_computer\n",
    "    \n",
    "    import numpy as np\n",
    "    os.environ['USE_PYGEOS'] = '0'\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    from pathlib import Path\n",
    "    from shapely.ops import transform\n",
    "    from azure.storage.blob import ContainerClient\n",
    "\n",
    "    container_client = ContainerClient(\n",
    "            os.getenv('AZURE_STORAGE_CONTAINER_URL'),\n",
    "            container_name=os.getenv('AZURE_STORAGE_CONTAINER_NAME'),\n",
    "            credential=os.getenv('AZURE_STORAGE_CONTAINER_TOK'),\n",
    "        )\n",
    "    \n",
    "    output_paths = []    \n",
    "    for payload in payloads:\n",
    "\n",
    "        geom = gpd.read_file('/tmp/aoi.geojson', driver='GeoJSON').iloc[0].geometry\n",
    "        \n",
    "        query_time_range = payload['query-date']\n",
    "    \n",
    "        catalog = pystac_client.Client.open(\n",
    "            payload['catalog_url'],\n",
    "            modifier=planetary_computer.sign_inplace\n",
    "        )\n",
    "    \n",
    "        query = catalog.search(\n",
    "            collections=payload['collection'],\n",
    "            intersects=geom.envelope,\n",
    "            datetime=query_time_range\n",
    "        )\n",
    "        \n",
    "        data = odc.stac.load(\n",
    "            query.items(),\n",
    "            bands=payload['assets'],\n",
    "        )\n",
    "        _data = data['NDSI_Snow_Cover']\n",
    "        _data.rio.set_nodata = 255\n",
    "        uint8_data = _data.astype(np.uint8)\n",
    "        modis_crs = pyproj.CRS.from_proj4('+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs')\n",
    "        uint8_data = uint8_data.rio.write_crs(modis_crs)\n",
    "        reprojected_data = uint8_data.rio.reproject(\n",
    "            dst_crs=f\"EPSG:{payload['epsg']}\",\n",
    "            resolution=payload['resolution'],\n",
    "            nodata=255\n",
    "        )\n",
    "            \n",
    "        if isinstance(reprojected_data, xarray.Dataset):\n",
    "            reprojected_data = reprojected_data.to_array(dim='band')\n",
    "        reprojected_data.name = query_time_range\n",
    "        reprojected_data.attrs['long_name'] = query_time_range\n",
    "\n",
    "        wgs84_poly = geom\n",
    "        wgs84 = pyproj.CRS('EPSG:4326')\n",
    "        utm = reprojected_data.rio.crs\n",
    "        projection_op = pyproj.Transformer.from_crs(wgs84, utm, always_xy=True).transform\n",
    "        utm_poly = transform(projection_op, wgs84_poly)\n",
    "        \n",
    "        # Clip data to reduce size and isolate to AOI\n",
    "        clipped_data = reprojected_data.rio.clip(\n",
    "                                        [utm_poly],\n",
    "                                        all_touched=True,\n",
    "                                        drop=True\n",
    "                                    )\n",
    "        \n",
    "        # Write Raster\n",
    "        filename = Path(query_time_range).stem\n",
    "        if not isinstance(filename, str):\n",
    "            filename = filename.as_posix()\n",
    "        filename += '.tif'\n",
    "        \n",
    "        output_path = (Path(payload['output_prefix']) / \n",
    "                       Path(''.join(payload['prov'].split(' '))) / \n",
    "                       Path(filename)\n",
    "                       ).as_posix()\n",
    "        \n",
    "        \n",
    "        blob_client = container_client.get_blob_client(output_path)\n",
    "    \n",
    "        with io.BytesIO() as buf:\n",
    "            clipped_data.rio.to_raster(buf, driver='GTiff')\n",
    "            buf.seek(0)\n",
    "            blob_client.upload_blob(buf, overwrite=True)\n",
    "        output_paths.append(output_path)\n",
    "    json.dump(output_paths, sys.stdout)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cda3a7-1daf-44e0-b617-e5b454dc3d4d",
   "metadata": {},
   "source": [
    "As a convenience, subsequent products can be derived from the preprocessed data. The following Task implements the creation of a preview PNG that can be used in reporting, documentation, or for STAC servers, to name only a few. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9678ba29-4adb-4664-b419-884e69af12a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T23:53:41.481405Z",
     "iopub.status.busy": "2023-08-29T23:53:41.480593Z",
     "iopub.status.idle": "2023-08-29T23:53:41.491507Z",
     "shell.execute_reply": "2023-08-29T23:53:41.490125Z",
     "shell.execute_reply.started": "2023-08-29T23:53:41.481377Z"
    }
   },
   "outputs": [],
   "source": [
    "@script(\n",
    "    image=docker_image_tag,\n",
    "    resources=Resources(cpu_request='500m', memory_request='4Gi'),\n",
    "    node_selector=pipeline_node_config['small']['nodeSelector'],\n",
    "    tolerations=pipeline_node_config['small']['tolerations'],\n",
    ")\n",
    "def create_preview_png(raster_paths: List[Any]):\n",
    "    import os\n",
    "    import io\n",
    "    import sys\n",
    "    import json\n",
    "    import xarray\n",
    "    import rioxarray\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    from PIL import Image\n",
    "    from pathlib import Path\n",
    "    from azure.storage.blob import ContainerClient\n",
    "\n",
    "    container_client = ContainerClient(\n",
    "            os.getenv('AZURE_STORAGE_CONTAINER_URL'),\n",
    "            container_name=os.getenv('AZURE_STORAGE_CONTAINER_NAME'),\n",
    "            credential=os.getenv('AZURE_STORAGE_CONTAINER_TOK'),\n",
    "        )\n",
    "    \n",
    "    for raster_path in raster_paths:\n",
    "        output_path = raster_path.replace('.tif', '.png')\n",
    "        raster_blob_client = container_client.get_blob_client(raster_path)\n",
    "        buf = io.BytesIO()\n",
    "        raster_blob_client.download_blob().readinto(buf)\n",
    "        buf.seek(0)\n",
    "        src = rioxarray.open_rasterio(buf)\n",
    "        data = src.data\n",
    "        if not isinstance(data, np.ndarray):\n",
    "            data = data.compute()\n",
    "        if data.shape[0] == 1:\n",
    "            data = data[0]\n",
    "        else:\n",
    "            data = data.transpose(1,2,0)\n",
    "        png_buf = io.BytesIO()\n",
    "        img = Image.fromarray(data)\n",
    "        blob_client = container_client.get_blob_client(output_path)\n",
    "        img.save(png_buf, format='PNG')\n",
    "        png_buf.seek(0)\n",
    "        blob_client.upload_blob(png_buf, overwrite=True)\n",
    "        buf.close()\n",
    "        src.close()\n",
    "        png_buf.close()\n",
    "    \n",
    "    json.dump(raster_paths, sys.stdout)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0ab1f-f316-45cd-8125-0d7288d5c3d7",
   "metadata": {},
   "source": [
    "## Main Workflow Entrypoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "726b3c1a-eef5-467c-ad2d-70a7aa9d8a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T00:07:29.451834Z",
     "iopub.status.busy": "2023-08-30T00:07:29.451101Z",
     "iopub.status.idle": "2023-08-30T00:07:29.457225Z",
     "shell.execute_reply": "2023-08-30T00:07:29.456479Z",
     "shell.execute_reply.started": "2023-08-30T00:07:29.451811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Workflow Configuration Section\n",
    "# -------------------------------\n",
    "\n",
    "# URL containing the Canadian Provincial Administrative boundary shapes.\n",
    "canada_boundaries_endpoint = 'https://ftp.maps.canada.ca/pub/nrcan_rncan/vector/canvec/shp/Admin/canvec_15M_CA_Admin_shp.zip'\n",
    "\n",
    "# List of Canadian Provinces in a left-to-right then up and back order\n",
    "target_provinces = [\n",
    "    'British Columbia',               # 0\n",
    "    'Alberta',                        # 1\n",
    "    'Saskatchewan',                   # 2\n",
    "    'Manitoba',                       # 3\n",
    "    'Ontario',                        # 4\n",
    "    'Quebec',                         # 5\n",
    "    'New Brunswick',                  # 6\n",
    "    'Nova Scotia',                    # 7\n",
    "    'Prince Edward Island',           # 8\n",
    "    'Newfoundland and Labrador',      # 9\n",
    "    'Nunavut',                        # 10\n",
    "    'Northwest Territories',          # 11\n",
    "    'Yukon'                           # 12\n",
    "]\n",
    "target_province = target_provinces[4]\n",
    "\n",
    "# Planetary Computer STAC Configuration\n",
    "catalog_url = 'https://planetarycomputer.microsoft.com/api/stac/v1'\n",
    "collection = 'modis-10A1-061'\n",
    "assets = ['NDSI_Snow_Cover']\n",
    "years = [2018,2019,2020,2021,2022]\n",
    "months = [10, 11, 12, 1, 2, 3, 4]\n",
    "\n",
    "# Data Configuration\n",
    "epsg = 3005\n",
    "resolution = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "217f2ea1-8b49-4c16-8410-a42f0aa27309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-30T00:07:30.798260Z",
     "iopub.status.busy": "2023-08-30T00:07:30.797530Z",
     "iopub.status.idle": "2023-08-30T00:07:30.811507Z",
     "shell.execute_reply": "2023-08-30T00:07:30.810899Z",
     "shell.execute_reply.started": "2023-08-30T00:07:30.798235Z"
    }
   },
   "outputs": [],
   "source": [
    "with Workflow(\n",
    "    name=f\"ingest-pc-stac-{collection.lower()}-{target_province.replace(' ', '').strip().lower()}\",\n",
    "    namespace=os.getenv(\"WORKFLOW_NS\"),\n",
    "    entrypoint=\"data-pipe\",\n",
    "    parallelism=20\n",
    ") as wf1:\n",
    "    \n",
    "    with DAG(\n",
    "        name='data-pipe', \n",
    "        parallelism=min(10, len(years)*len(months)),\n",
    "    ) as _d:\n",
    "        a0 = generate_polygon(\n",
    "                arguments={\n",
    "                    'url': canada_boundaries_endpoint,\n",
    "                    'target_province': target_province\n",
    "                },\n",
    "        )\n",
    "        b0 = fan_out_temporal_range(\n",
    "                arguments={\n",
    "                    'catalog_url': catalog_url,\n",
    "                    'collection': collection,\n",
    "                    'assets': assets,                               \n",
    "                    'years': years,\n",
    "                    'months': months,\n",
    "                    'epsg': epsg,\n",
    "                    'resolution': resolution,\n",
    "                    'target_province': target_province,\n",
    "                    'output_prefix': 'modis_daily_snow'\n",
    "                }\n",
    "            )\n",
    "\n",
    "        a1 = query_for_data(\n",
    "            arguments=[a0.get_artifact('aoi').as_name('aoi')],\n",
    "            with_param=b0.result,\n",
    "        )\n",
    "\n",
    "        a2 = process_daily_granules(\n",
    "            arguments=[\n",
    "                a0.get_artifact('aoi').as_name('aoi'),\n",
    "            ],\n",
    "            with_param=a1.result\n",
    "        )\n",
    "        b2 = create_preview_png(\n",
    "            with_param=a2.result\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        Finally, we declare our DAG.\n",
    "        We can run the fisrt two preparation Tasks in parallel and\n",
    "        then subsequent Tasks operate sequentially.\n",
    "        \"\"\"\n",
    "        [a0, b0] >> a1 >> a2 >> b2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee2669-5ef0-4213-9da9-ffc9331f723b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "wf1.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25326f21-afd9-4815-9b3e-0a99509c0d57",
   "metadata": {},
   "source": [
    "## ARD Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c401f-ed23-4691-aa03-e5d9f9781f7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "932d1cbb-e149-4bc9-b7c5-fc2026903675",
   "metadata": {},
   "source": [
    "### NDSI_Snow_Cover Value Map\n",
    "\n",
    "- 0â€“100: NDSI snow cover\n",
    "- 200: missing data\n",
    "- 201: no decision\n",
    "- 211: night\n",
    "- 237: inland water\n",
    "- 239: ocean\n",
    "- 250: cloud\n",
    "- 254: detector saturated\n",
    "- 255: fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09cfd2-760f-4fd4-a291-5661f685e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import rioxarray\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from pystac_client import Client\n",
    "from dask_gateway import Gateway\n",
    "from collections import defaultdict\n",
    "from azure.storage.blob import ContainerClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f9fdb-dbf2-4042-9b7e-146007b488f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_or_update_stac_item(raster_path: str):\n",
    "    import io\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import pystac\n",
    "    import requests\n",
    "    \n",
    "    headers={'cookie': auth_token}\n",
    "    data = json.dumps(collection.to_dict())\n",
    "    r = requests.post('', data=data, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed5d6a-a81b-4cc9-bc85-ae6f759dc1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_update_stac_collection():\n",
    "    import json\n",
    "    import pystac\n",
    "    import requests\n",
    "\n",
    "    collection = pystac.Collection(\n",
    "                                    id='',\n",
    "                                    href='https://stac.geoanalytics.ca/collections',\n",
    "                                    title='',\n",
    "                                    description='',\n",
    "                                    extent='',\n",
    "                                    keywords=[''],\n",
    "                                    license='',\n",
    "                                  )\n",
    "    collection.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c7cbf-885b-4c54-9ac9-7fa67e6bd18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_client = ContainerClient(\n",
    "    os.getenv('AZURE_STORAGE_CONTAINER_URL'),\n",
    "    container_name=os.getenv('AZURE_STORAGE_CONTAINER_NAME'),\n",
    "    credential=os.getenv('AZURE_STORAGE_CONTAINER_TOK'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e53f3f-7a7a-4867-80f8-d3a791db4f02",
   "metadata": {},
   "source": [
    "Isolate in own cell so it can be run only once.  \n",
    "The gateway object can be used to connect to and create other Dask Clusters.  \n",
    "See [our documentation](https://docs.geoanalytics.ca/1_getting_started/10-dask.html) for more information about Dask in GEOAnalytics Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bbc7b-6ea9-47c3-9f8f-e4a6cbad389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = Gateway()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0204d7e-515c-4d58-8b41-acaf127e1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "cluster_name = 'daskhub.f6c26b31cafd48edb54593017ced7703'\n",
    "\n",
    "geoanalytics_stac_url = 'https://stac.eo4ph.geoanalytics.ca'\n",
    "stac_url = 'https://stac.geoanalytics.ca/collections/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f0fce-c8c3-439b-8065-b35b00cb3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster = gateway.connect(cluster_name)\n",
    "dask_client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c60f6-c276-4bc0-a94d-62074b37e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72642235-bb11-4231-adb2-a7b5969a6a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blobs = container_client.list_blobs(name_starts_with='modis_daily_snow/BritishColumbia/')\n",
    "raster_map = defaultdict(list)\n",
    "for blob in blobs:\n",
    "    if '.ipynb_checkpoints' in blob.name:\n",
    "        container_client.delete_blob(blob)\n",
    "        continue\n",
    "    if blob.name.endswith('.png'):\n",
    "        continue\n",
    "    pth = Path(blob.name)\n",
    "    map_key = '-'.join(pth.stem.split('-')[:2])\n",
    "    raster_map[map_key].append(blob.name)\n",
    "raster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8bfbb8-e5a0-4c26-8b35-3ecacd8b0bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raster_month_map = {}\n",
    "attr_copy = None\n",
    "for k in raster_map.keys():\n",
    "    _monthly_data = raster_map[k]\n",
    "    # Open rasters into the dask cluster\n",
    "    open_rasters = [\n",
    "        rioxarray.open_rasterio(\n",
    "            container_client.get_blob_client(_blob).url, chunks='auto')\n",
    "        for _blob in _monthly_data]\n",
    "    if attr_copy is None:\n",
    "        attr_copy = open_rasters[0].attrs\n",
    "        attr_copy['long_name'] = 'MODIS Snow Cover'\n",
    "    month_data = xarray.concat(open_rasters, dim='time')\n",
    "    month_data = month_data.chunk('auto') # rechunk at dimension changes\n",
    "    filtered_month_data = xarray.where((month_data >100), np.nan, month_data, keep_attrs=True)\n",
    "    median_month_data = filtered_month_data.median(dim='time', skipna=True, keep_attrs=True)\n",
    "    median_month_data = median_month_data.chunk('auto')\n",
    "    final_data = xarray.where(np.isnan(median_month_data), 255, median_month_data).astype(np.uint8)\n",
    "    raster_month_map[k] = final_data\n",
    "ds = xarray.Dataset(data_vars=raster_month_map, attrs=attr_copy)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f829110-2a2a-4265-9ec3-2b09adfc05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[0].plot.imshow(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6c02c-fdc4-4ea8-aff5-5d7e13d82c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9b02a9-4cf8-478e-ba74-a99b840ec780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
