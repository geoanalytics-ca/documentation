{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711d5866",
   "metadata": {},
   "source": [
    "# **Pipelines**\n",
    "\n",
    "## 1. Basics of GEOAnalytics Canada Pipelines\n",
    "\n",
    "Kubeflow Pipelines is a platform that provides tools for developing and building portable, scalable machine learning (ML) workflows based on Docker containers.\n",
    "\n",
    "**The Kubeflow Pipelines platform consists of:**\n",
    "\n",
    "* A UI for managing and tracking pipelines and their execution\n",
    "* An engine for scheduling a pipeline’s execution\n",
    "* An SDK for defining, building, and deploying pipelines in Python\n",
    "\n",
    "A pipeline is a representation of a ML workflow containing the parameters required to run the pipeline and the inputs and outputs of each component. Each pipeline component is a self-contained code block, packaged as a Docker image.\n",
    "\n",
    "\n",
    "In this tutorial notebook, we will build our first Kubeflow Pipelines with Python functions converted to container components and with Docker images. First, run the following command to install all the packages and dependencies required for this tutorial. The requirements.txt file contains the Kubeflow Pipelines SDK and `lxml` (a Python library which allows for easy handling of XML and HTML files, and can also be used for web scraping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86861e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r data/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d2d90a",
   "metadata": {},
   "source": [
    "## 2. Building Pipelines in Jupyterhub\n",
    "In this section we will connect to the GEOAnalytics Kubeflow Client where our Pipeline will be uploaded, and create, compile, and run a simple 'Hello World!' pipeline. \n",
    "\n",
    "A regular pattern for building pipelines in Kubeflow Pipelines is:\n",
    "\n",
    "1. Define components for each task\n",
    "2. Stitch the components in a `@dsl.pipeline` decorated function\n",
    "3. Compile the pipeline, upload the YAML file, and run the pipeline.\n",
    "\n",
    "This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. \n",
    "\n",
    "***Important Note: The Kubeflow platform provides Jupyter Notebook Servers for using the SDK and pipeline execution, but please DO NOT create a pipeline from these server notebooks.***\n",
    "\n",
    "### 2.1 Connect to GEOAnalytics Pipeline Client¶\n",
    "The first step is to get connected with the the Kubeflow Client in GEOAnalytics Canada.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3472dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GEOAnalyticsKubeflowClient\n",
    "import requests\n",
    "import kfp\n",
    "import json\n",
    "import logging\n",
    "import getpass\n",
    "\n",
    "from lxml import html\n",
    "\n",
    "#from auth_providers.DexProvider import DexProvider\n",
    "\n",
    "class GEOAnalyticsKubeflowClient:\n",
    "    def __init__(self, username, password, auth_provider, namespace=None):\n",
    "        self.logger = None\n",
    "        self._initialize_logger()\n",
    "\n",
    "        self.client = None\n",
    "        self.namespace = username if namespace is None else namespace\n",
    "        \n",
    "        self._authenticate(username, password, auth_provider)\n",
    "        self._validate_client_connection()\n",
    "\n",
    "    def _initialize_logger(self):\n",
    "        logging.basicConfig(format='%(asctime)s %(levelname)s (%(name)s): %(message)s', level=logging.INFO, datefmt='%Y-%m-%d %H:%M:%S')\n",
    "        self.logger = logging.getLogger(\"GEOAnalyticsKubeflowClient\")\n",
    "\n",
    "    # https://github.com/kubeflow/kfctl/issues/140#issuecomment-719894529\n",
    "    def _authenticate(self, username, password, auth_provider):\n",
    "        session = requests.Session()\n",
    "        response = session.get(auth_provider.host)\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        }\n",
    "\n",
    "        data = auth_provider.get_auth_data_dict(username, password)\n",
    "        post_url = auth_provider.get_auth_post_url(response)\n",
    "        session.post(post_url, headers=headers, data=data)\n",
    "        try:\n",
    "            session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "        except Exception as e:\n",
    "            message = \"invalid host or credentials\"\n",
    "            self.logger.error(message)\n",
    "            raise Exception(message) from None\n",
    "\n",
    "        self.client = kfp.Client(\n",
    "            host=f\"{auth_provider.host}/pipeline\",\n",
    "            cookies=f\"authservice_session={session_cookie}\",\n",
    "            namespace=self.namespace,\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _validate_client_connection(self):\n",
    "        if self.client.list_pipelines().total_size > 0:\n",
    "            self.logger.info(\"successfully authenticated with kubeflow\")\n",
    "        else:\n",
    "            message = \"unable to validate kubeflow client connection. listing pipelines failed.\"\n",
    "            self.logger.error(message)\n",
    "            raise Exception(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83f3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DexProvider:\n",
    "    def __init__(self, host):\n",
    "        self.host = host\n",
    "        self.name = \"dex\"\n",
    "\n",
    "    def get_auth_data_dict(self, username, password):\n",
    "        return {\"login\": username, \"password\": password}\n",
    "\n",
    "    def get_auth_post_url(self, initial_response):\n",
    "        return initial_response.url\n",
    "\n",
    "    def get_provider_name(self):\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81a195",
   "metadata": {},
   "source": [
    "The following cell block takes in the URL to the GEOAnalytics Kubeflow server as the provider and requires you to enter your GEOAnalytics Canada username and password to gain authentication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7437fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  asaini\n",
      " ············\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-25 21:54:16 INFO (GEOAnalyticsKubeflowClient): successfully authenticated with kubeflow\n"
     ]
    }
   ],
   "source": [
    "provider = DexProvider(\"http://kubeflow.geoanalytics.ca\")\n",
    "kubeflow_client = GEOAnalyticsKubeflowClient(input(\"Username: \"), getpass.getpass(), provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732a957",
   "metadata": {},
   "source": [
    "Now that we are authenticated, let's build our pipelines.\n",
    "There are two ways components of a pipeline can be generated: Using python functions or using docker images. We will create two pipelines experimenting with both these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cb910",
   "metadata": {},
   "source": [
    "### 2.2 Building Pipelines through Python Function Based Components\n",
    "1. **Define a standard python function**: For our example, the function will combine two separate strings together. The function should not use any code declared outside of the function definition and helper function must be declared within the function. The input and output data type should also be explicitly declared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f15dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669b335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two strings together\n",
    "def combine(a: str, b: str) -> str:\n",
    "    print(a + b)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe906d5e",
   "metadata": {},
   "source": [
    "2. **Wrap the function in a Kubeflow lightweight component**: the Kubeflow Python SDK allows you to build lightweight components by defining python functions and converting them using `kfp.components.create_component_from_func(<python function>)`. Specifying a custom Docker container image can be provided as a parameter of this function. The default base image is *python3.7*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533067d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combine_op = kfp.components.create_component_from_func(combine) # default base_image is python3.7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd140a",
   "metadata": {},
   "source": [
    "3. **Build a Kubeflow Pipeline using the component**: You can use the component created above in a pipeline. The parameters of `@dsl.pipeline` define the metadata of the pipeline. The combine_strings_pipeline has a decorator function with the goal of printing \"Hello World!\" as the final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3581ee1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Hello World pipeline',\n",
    "  description='An easy example pipeline that combines strings together to create the Hello World phrase.'\n",
    ")\n",
    "def combine_strings_pipeline(a='Hello', b='World'):  \n",
    "    # Passing in the first pipeline parameter ('Hello') and a space to be combined by the  `combine_op` function.\n",
    "    add_space = combine_op(a, ' ')\n",
    "    \n",
    "    # Passes an output reference from `add_space` and the next pipeline parameter ('World')\n",
    "    combine_words = combine_op(add_space.output, b)\n",
    "  \n",
    "    # Adding an exclamation to the output reference from `combine_words`\n",
    "    add_exclamation = combine_op(combine_words.output, '!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5270bcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. **Compiling the Kubeflow Pipeline**: Here we compile the pipeline function into a YAML file that Kubeflow Pipelines can use. A pipeline name, and the pipeline function to be processed are inputs for the compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149dad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE PIPELINE #\n",
    "pipeline_name = \"Hello World Pipeline!\"\n",
    "pipeline_func = combine_strings_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.yaml'\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0cf4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {'a': 'Hello', 'b': 'World'} # pipeline argument values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a521841",
   "metadata": {},
   "source": [
    "4. **Run the experiments**: Kubeflow Pipelines lets you group pipeline runs by ***Experiments***. Experiments are workspaces where you can try different configurations of your pipelines. You can use experiments to organize your runs into logical groups. \n",
    "\n",
    "- In this code block below, we create an experiment with the name \"combine-hello-world\" and label the run iteration manually. The `.run_pipeline(<parameters>)` function runs our specifc pipeline, outputting the hyperlinks to the Experiment and Run details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1fa653e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-25 21:54:17 INFO (root): Creating experiment combine-hello-world.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://kubeflow.geoanalytics.ca/pipeline/#/experiments/details/5a34b033-4187-44f0-b851-10498d3b42ea\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://kubeflow.geoanalytics.ca/pipeline/#/runs/details/f0617ebf-64e1-4c7e-9447-c421f46f29b8\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If the experiment under the given name already exists - get it's ID, else - create a new experiment\n",
    "try:\n",
    "    experiment_id = kubeflow_client.client.get_experiment(\"combine-hello-world\")\n",
    "except:\n",
    "    experiment = kubeflow_client.client.create_experiment(\"combine-hello-world\")\n",
    "    \n",
    "run_name = pipeline_func.__name__ + '-run#1'\n",
    "run_result = kubeflow_client.client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f3d9e",
   "metadata": {},
   "source": [
    "5. **Upload to the Kubeflow Pipelines UI**: After developing your pipeline,the pipeline can be shared on the Kubeflow Pipelines UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f896dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=http://kubeflow.geoanalytics.ca/pipeline/#/pipelines/details/968ad52f-71a0-4109-b72f-e06740c4388e>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combine_hw_pipeline = kubeflow_client.client.upload_pipeline(pipeline_filename, pipeline_name, \"'Hello World!' created by the combine_strings pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dec3c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': datetime.datetime(2021, 6, 25, 21, 54, 17, tzinfo=tzlocal()),\n",
       " 'default_version': {'code_source_url': None,\n",
       "                     'created_at': datetime.datetime(2021, 6, 25, 21, 54, 17, tzinfo=tzlocal()),\n",
       "                     'id': '968ad52f-71a0-4109-b72f-e06740c4388e',\n",
       "                     'name': 'Hello World Pipeline!',\n",
       "                     'package_url': None,\n",
       "                     'parameters': [{'name': 'a', 'value': 'Hello'},\n",
       "                                    {'name': 'b', 'value': 'World'}],\n",
       "                     'resource_references': [{'key': {'id': '968ad52f-71a0-4109-b72f-e06740c4388e',\n",
       "                                                      'type': 'PIPELINE'},\n",
       "                                              'name': None,\n",
       "                                              'relationship': 'OWNER'}]},\n",
       " 'description': \"'Hello World!' created by the combine_strings pipeline!\",\n",
       " 'error': None,\n",
       " 'id': '968ad52f-71a0-4109-b72f-e06740c4388e',\n",
       " 'name': 'Hello World Pipeline!',\n",
       " 'parameters': [{'name': 'a', 'value': 'Hello'},\n",
       "                {'name': 'b', 'value': 'World'}],\n",
       " 'url': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_hw_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3dc68",
   "metadata": {},
   "source": [
    "### 2.3 Building Pipelines by Docker Images \n",
    "Now that we have seen how lightweight containers are used in pipelines, we will explore how to build a pipeline using a docker container image. When Kubeflow Pipelines executes a component, a container image is started in a Kubernetes Pod and your component’s inputs are passed in as command-line arguments. \n",
    "\n",
    "Before we can build our pipeline, let's get our docker image ready for use. For our example, we will be using 'library/bash:4.4.23' image, publicly available on DockerHub. Using the same techniques from the **Gitlab** tutorial notebook, push this image to a private container registry. \n",
    "\n",
    "- **Hint:** Rename the image to `registry.geoanalytics.ca/<username>/bash-library`\n",
    "\n",
    "Now let's talk run the cell below to create an echo operation to be called within our pipeline. This function will return an operation that runs an image. `dsl.ContainerOp()` takes in multiple parameters, but for our example we will focus on only these four:\n",
    "\n",
    "- `name` – the name of the operation, which is 'echo' in this case. \n",
    "- `image` – the name of our container image.\n",
    "- `command` – the command to run in the container. **'sh'** invokes the default shell as interpreter and the **'-c'** flag means execute the following command as interpreted by this program.\n",
    "- `arguments` – the arguments of the command. At container run time the argument will be `echo \"hello world\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1abef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_op():\n",
    "    return dsl.ContainerOp(\n",
    "        name='echo',\n",
    "        image='registry.geoanalytics.ca/examples/bash-library',\n",
    "        command=['sh', '-c'],\n",
    "        arguments=['echo \"hello world\"']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd577ee5",
   "metadata": {},
   "source": [
    "This python function directly returns a container operation and can be used in the pipeline. This means in contrast to the previous Python function (combine_op), there is no need to wrap the function as a component for it be processed in a pipeline.\n",
    "\n",
    "The function below builds a Kubeflow Pipeline using the echo operation, similar to the lightweight function pipeline. \n",
    "The pipeline function calls our echo component as the main task. The image to be used exists in a private container registry which requires additional authentication steps. The `dsl.get_pipeline_conf()` allows configuration of the pipeline and uses the `.set_image_pull_secrets(...)` to authenticate the docker image using the secret name \"regcredgeoanalytics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cf16ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Echo Image pipeline',\n",
    "    description='An example pipeline that echos \"hello world\" in the container image.'\n",
    ")\n",
    "def echo_hello_world_pipeline():\n",
    "    echo_task = echo_op()\n",
    "    dsl.get_pipeline_conf().set_image_pull_secrets([client.V1LocalObjectReference(name=\"regcredgeoanalytics\")]) # access to your registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91224c",
   "metadata": {},
   "source": [
    "Compile, run, and upload your pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1733f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.8/site-packages/kfp/dsl/_container_op.py:1021: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# COMPILE PIPELINE #\n",
    "pipeline_name = \"Echo Image Pipeline\"\n",
    "pipeline_func = echo_hello_world_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.yaml'\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ac055af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-25 21:54:17 INFO (root): Creating experiment echo-image.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://kubeflow.geoanalytics.ca/pipeline/#/experiments/details/7ab33a7e-97d6-4435-9b46-59d97b13e840\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://kubeflow.geoanalytics.ca/pipeline/#/runs/details/67833d3e-0926-41f4-91d3-9683d496925e\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If the experiment under the given name already exists - get it's ID, else - create a new experiment\n",
    "try:\n",
    "    experiment_id = kubeflow_client.client.get_experiment(\"echo-image\")\n",
    "except:\n",
    "    experiment = kubeflow_client.client.create_experiment(\"echo-image\")\n",
    "    \n",
    "# RUN THE PIPELINE #\n",
    "run_name = pipeline_func.__name__ + '-run#1'\n",
    "arguments = {'arg': 'echo \"hello world\"'} # passing in the arguments \n",
    "run_result = kubeflow_client.client.run_pipeline(experiment.id, run_name, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff57edd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=http://kubeflow.geoanalytics.ca/pipeline/#/pipelines/details/ad822c7a-1103-4a0c-86c1-1337bbb34461>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "echo_pipeline = kubeflow_client.client.upload_pipeline(pipeline_filename, pipeline_name, \"ECHO IMAGE PIPELINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3acc37",
   "metadata": {},
   "source": [
    "## 3. Exploring the GEOAnalytics Pipeline UI\n",
    "### 3.1 Kubeflow Pipelines and Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b4c3f",
   "metadata": {},
   "source": [
    "The central Kubeflow Dashboard can be accessed from the main GEOAnalytics Canada page by clicking on Kubeflow to launch.\n",
    "<img src='../images/getting_started_images/08_launching-kubeflow.gif'>\n",
    "\n",
    "Now you will have access to the Kubeflow User Interface (UI). \n",
    "\n",
    "\n",
    "On the left sidebar, click on the **Pipelines** tab which will lead to the main workspace for Kubeflow Pipelines UI. On the left panel in this section, there are multiple tabs, but for this tutorial we will focus on only the Pipelines and Experiments tab. The *Pipelines* tab leads you to all the existing pipelines you have access to and the option to upload a new pipeline. And the *Experiments* tab displays all the experients and the runs of each experiment. You can create runs, compare existing runs, or even clone the runs.\n",
    "\n",
    "You are welcome to experiment with running pipelines to get a better feel for their execution and the capabilities of the Pipelines UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafc8b7",
   "metadata": {},
   "source": [
    "### 3.2 Analyzing the Hello World Pipeline\n",
    "\n",
    "Under the Pipelines tab, you should see the newly built \"Hello World Example Pipeline\" at the top of the list of pipelines. There are other Tutorial and Demo pipelines preconfigured by Kubeflow for you to experiment with. \n",
    "\n",
    "To invoke a specific pipeline, simply click it which will bring up Pipeline’s view as shown in the GIF below. \n",
    "\n",
    "<img src='../images/getting_started_images/08_kubeflow-pipelines.gif'>\n",
    "\n",
    "Within this pipeline there is a graph representing the runtime execution of the pipeline, with edges indicating parent/child relationships between the steps (nodes). Clicking on a node will give information about that step, such as the input parameter, the output result, arguments, internal commands to execute that step, and the volume mounts. \n",
    "\n",
    "**Notebook and the pipeline run independently, so in case the notebook crashes, the experiment will continue running once executed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bea862",
   "metadata": {},
   "source": [
    "### 3.3 Analyzing the Docker Image Pipeline\n",
    "\n",
    "Take the same steps to view the information about our Echo Image Pipeline. In the graph there is only one node, \"echo.\" As you can note, the arguments, commands, at the image are the same ones we specified in the `ContainerOp`.\n",
    "\n",
    "There is also another tab next to Graph, called **YAML**. This will show our pipeline filename we created when the pipeline was compiled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cefee2",
   "metadata": {},
   "source": [
    "### 3.4 Create a New Run through the UI\n",
    "While you can create a run through Jupyter notebooks, it can also be done directly through the UI if the pipeline is already uploaded. \n",
    "The image below shows the Kubeflow Pipelines UI for starting a run of the \"Hello World Pipeline\". To run the pipeline, click the **“Create Run”** button and fill in the run details on the screen.\n",
    "\n",
    "\n",
    "Filling in the Run Details:\n",
    "\n",
    "- The *Pipeline* and *Pipeline Version* should already be set to the current pipeline, \"Hello World Pipeline!\"\n",
    "- The *Run name* can be set to anything you prefer, identifying the unique run. \n",
    "- The *Description* is an option to describe the details of the current run.\n",
    "- The *Experiment* to be associated with the run must be selected from the list of existing experiments, in our case it should be the **\"combine-hello-world\"**.\n",
    "- There are two different types of Runs, we will discuss these in the next part. For now, select **\"One-off\"** which executes the pipeline only once. \n",
    "- Now since we specified *Parameters* in our pipeline function, enter any two words in place of `a` and `b`.\n",
    "\n",
    "<img src='../images/getting_started_images/08_start-run.png'>\n",
    "\n",
    "After these sections have been filled in, select the **\"Start\"** to execute the run which will take a few seconds (or minutes depending on the pipeline). When the Status updates to a green checkmark it indicates the run was successfully executed. Click on the run to explore the details of the run. \n",
    "\n",
    "<img src='../images/getting_started_images/08_run-created.gif'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9d2ad",
   "metadata": {},
   "source": [
    "### 3.5 Running Pipelines on Schedule\n",
    "All our previous runs have been done manually, which is not ideal for some scenarios. Recurring runs are repeatable runs of a pipeline, very helpful if the pipeline is expected to run for a long period of time and is triggered to run frequently.\n",
    "\n",
    "Let's create a Recurring run on our \"Echo Image Pipeline.\" Fill in the details as shown in the screenshot below. Now  you can create a periodic run by selecting a run type of **“Recurring”**, then choosing **\"Periodic\"** as the trigger type. Period runs specify ***how often*** to run a pipeline. In this example we are setting a pipeline to run every second day. Once you start the run, it is executed immediately and then executed with the defined frequency (of 2 days). \n",
    "\n",
    "<img src='../images/getting_started_images/08_create-recurring-run.png'>\n",
    "\n",
    "From the \"echo-image\" experiment, you will see recurring run active, with the option to `enable` or `disable` the schedule. \n",
    "\n",
    "**If you enable a recurring run for this example notebook, make sure to disable the run.**\n",
    "\n",
    "<img src='../images/getting_started_images/08_recurring-run.gif'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e487fe",
   "metadata": {},
   "source": [
    "### 3.6 Terminating Runs\n",
    "\n",
    "In the case where the run takes a while to execute and you want to stop the run, it can be terminated through the UI. Once you click on the ongoing run, you will see a **\"Terminate\"** button in the top right of the page. This will immediately terminate the execution of that run, but it will not delete the record of the run. \n",
    "\n",
    "The next section will show how to delete runs, experiments, and pipelines in a Jupyter notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f93b5",
   "metadata": {},
   "source": [
    "## 4. Deleting Runs, Experiments, and Pipelines\n",
    "To keep your environment clean, let's delete the runs, experiments, and pipelines we created in this notebook. (You can choose to keep them if you prefer!)\n",
    "\n",
    "### 4.1 Delete all the runs \n",
    "The following code blocks show how to delete runs of a particular experiments. \n",
    "\n",
    "We use `.get_experiment(<name>)` to get the experiment id which will be used to find all the runs within this experiment. Let's start by getting the experiment information for the \"echo-image\" and the \"combine-hello-world image\" experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afc1fbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': datetime.datetime(2021, 6, 25, 21, 54, 17, tzinfo=tzlocal()),\n",
       " 'description': None,\n",
       " 'id': '7ab33a7e-97d6-4435-9b46-59d97b13e840',\n",
       " 'name': 'echo-image',\n",
       " 'resource_references': None,\n",
       " 'storage_state': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echo_experiment = kubeflow_client.client.get_experiment(experiment_name='echo-image')\n",
    "echo_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c02f4ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': datetime.datetime(2021, 6, 25, 21, 54, 17, tzinfo=tzlocal()),\n",
       " 'description': None,\n",
       " 'id': '5a34b033-4187-44f0-b851-10498d3b42ea',\n",
       " 'name': 'combine-hello-world',\n",
       " 'resource_references': None,\n",
       " 'storage_state': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_hw_experiment = kubeflow_client.client.get_experiment(experiment_name='combine-hello-world')\n",
    "combine_hw_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00130d5d",
   "metadata": {},
   "source": [
    "Now that we have the the information for our experiments in a dictionary, we can use the 'id' key to delete the experiments' runs.\n",
    "The cells below collects information for all the runs of each experiment, and iteratively deletes each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3314895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f0617ebf-64e1-4c7e-9447-c421f46f29b8\n"
     ]
    }
   ],
   "source": [
    "combine_hw_runs = kubeflow_client.client.list_runs(experiment_id=combine_hw_experiment.id)\n",
    "# Delete all runs from the \"combine-hello-world\" experiment\n",
    "for i in range(len(combine_hw_runs.runs)):\n",
    "    print(combine_hw_runs.runs[i].id)\n",
    "    kubeflow_client.client.runs.delete_run(combine_hw_runs.runs[i].id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b50df8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67833d3e-0926-41f4-91d3-9683d496925e\n"
     ]
    }
   ],
   "source": [
    "echo_runs = kubeflow_client.client.list_runs(experiment_id=echo_experiment.id)\n",
    "# Delete all runs from the \"combine-hello-world\" experiment\n",
    "for i in range(len(echo_runs.runs)):\n",
    "    print(echo_runs.runs[i].id)\n",
    "    kubeflow_client.client.runs.delete_run(echo_runs.runs[i].id) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6854c90",
   "metadata": {},
   "source": [
    "Now, if you head over to the Kubeflow Pipeline UI and take a look at the experiments, there should be no runs available anymore. \n",
    "\n",
    "### 4.2 Delete the experiments\n",
    "Since the runs are deleted, we can safely delete the experiments without worrying about unexpected behaviors.\n",
    "\n",
    "We already have the experiment IDs which will be the parameter input for the function `.experiments.delete_experiments(<experiment_id>)`. Once you have run the cell below, refresh your Experiments page on the UI to see these experiments disappear from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb096a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kubeflow_client.client.experiments.delete_experiment(combine_hw_experiment.id) # deletes combine-hello-world experiment\n",
    "kubeflow_client.client.experiments.delete_experiment(echo_experiment.id) # deletes echo-image experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d521d",
   "metadata": {},
   "source": [
    "### 4.3 Delete the pipelines\n",
    "Pipelines can be deleted through Jupyter *and* through the Kubeflow Pipeline UI!\n",
    "\n",
    "To delete through the UI, simply tick off the boxes next to the pipelines and select the **Delete** button to delete them.\n",
    "\n",
    "There are different steps for deleting through a Jupyter notebook. When we uploaded the combine_strings pipeline and echo_image pipeline, we assigned the information of the pipelines to variables **combine_hw_pipeline** and **echo_pipeline**, respectively. \n",
    "We will use the **'id'** keys from their pipeline information dictionaries (similar to how the experiment IDs were retrieved) as the parameter for identifying the pipelines we wish to delete, using the function `delete_pipeline(<pipeline_id>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d435829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kubeflow_client.client.delete_pipeline(combine_hw_pipeline.id) \n",
    "kubeflow_client.client.delete_pipeline(echo_pipeline.id) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
