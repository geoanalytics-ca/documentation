{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004f9a76-a752-4e84-9485-713124e8e0ea",
   "metadata": {},
   "source": [
    "# **Pipelines**\n",
    "\n",
    "## 1. Basics of GEOAnalytics Canada Pipelines\n",
    "\n",
    "The GeoAnalytics Canada Pipeline system helps with developing and building portable, scalable Earth Observation pre-processing pipelines and machine learning (ML) workflows based on Docker containers.\n",
    "\n",
    "**The Pipelines platform consists of:**\n",
    "\n",
    "* A UI for managing and tracking pipelines and their execution.  The [EO4PH Pipeline UI is available here](https://pipeline.eo4ph.geoanalytics.ca/)\n",
    "* An engine for scheduling a pipelineâ€™s execution\n",
    "* An SDK for defining, building, and deploying pipelines in Python. The SDK we use is the [Couler python library](https://couler-proj.github.io/couler/)\n",
    "\n",
    "A pipeline is a representation of a ML workflow containing the parameters required to run the pipeline and the inputs and outputs of each component. Each pipeline component is a self-contained code block, packaged as a Docker image.\n",
    "\n",
    "\n",
    "In this tutorial notebook, we will build our first Pipeline. \n",
    "\n",
    "First, run the following command to install all the packages and dependencies required for this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca62465",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install the Couler SDK from source\n",
    "!python3 -m pip install git+https://github.com/couler-proj/couler --ignore-installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ba386-e7ce-45f7-9d52-e5fcab35c7e4",
   "metadata": {},
   "source": [
    "## 2. Building A Basic Pipeline\n",
    "\n",
    "After installing the required dependencies for this tutorial, \n",
    "we then need to import the necessary modules. \n",
    "Next, we define a job template that pacakges each step into its own Container. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4346907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import couler.argo as couler\n",
    "\n",
    "from couler.argo_submitter import ArgoSubmitter\n",
    "from couler.core.templates.toleration import Toleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d08eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job(name):\n",
    "    # two tolerations are needed to ensure the job is scheduled on the correct GEOAnalytics Canada cluster nodes.\n",
    "    toleration = Toleration(os.getenv('WORKFLOW_NODE_SELECTOR_KEY'), 'NoSchedule', 'Exists')\n",
    "    couler.add_toleration(toleration) # pipeline/nodepool=pipe:NoSchedule\n",
    "    toleration2 = Toleration('kubernetes.azure.com/scalesetpriority', 'NoSchedule', 'Exists')\n",
    "    couler.add_toleration(toleration2)\n",
    "    # Set the container to run. You could use your own container hosted in the GEOAnalytics Canada container registry, or in this case docker/whalesay\n",
    "    # Documentation on the docker/whalesay image and arguments is here: https://hub.docker.com/r/docker/whalesay/ \n",
    "    couler.run_container(\n",
    "        image=\"docker/whalesay:latest\",\n",
    "        command=[\"cowsay\"],\n",
    "        # pass the name as an argument to the whalesay docker image\n",
    "        args=[name],\n",
    "        step_name=name,\n",
    "        node_selector={os.getenv('WORKFLOW_NODE_SELECTOR_KEY'):os.getenv('WORKFLOW_NODE_SELECTOR_VALUE')}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb5727-5571-4ca7-a9d6-e5967c372b8d",
   "metadata": {},
   "source": [
    "The next two functions demonstrate the dependencies between each pipeline step that \n",
    "can be created. Further down, we will see a more complex example, however,\n",
    "declaring simple dependencies such as these to block subsequent steps from\n",
    "operating before a given step has finished running can prove to be a powerful\n",
    "tool when building complex products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aae3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     A\n",
    "#    / \\\n",
    "#   B   C\n",
    "#  /\n",
    "# D\n",
    "def linear_pipeline():\n",
    "    couler.set_dependencies(lambda: job(name=\"A\"), dependencies=None)\n",
    "    couler.set_dependencies(lambda: job(name=\"B\"), dependencies=[\"A\"])\n",
    "    couler.set_dependencies(lambda: job(name=\"C\"), dependencies=[\"A\"])\n",
    "    couler.set_dependencies(lambda: job(name=\"D\"), dependencies=[\"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dce745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   A\n",
    "#  / \\\n",
    "# B   C\n",
    "#  \\ /\n",
    "#   D\n",
    "def diamond_pipeline():\n",
    "    couler.dag( # DAG: Directed Acyclic Graph (the same as a 'pipeline')\n",
    "        [\n",
    "            [lambda: job(name=\"A\")],\n",
    "            [lambda: job(name=\"A\"), lambda: job(name=\"B\")],  # A -> B\n",
    "            [lambda: job(name=\"A\"), lambda: job(name=\"C\")],  # A -> C\n",
    "            [lambda: job(name=\"B\"), lambda: job(name=\"D\")],  # B -> D\n",
    "            [lambda: job(name=\"C\"), lambda: job(name=\"D\")],  # C -> D\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f693b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_pipeline()\n",
    "diamond_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc0b54-1b9c-40ec-acbb-9d50e234c51a",
   "metadata": {},
   "source": [
    "We then will submit our job to the `pipeline` namespace where jobs will be run.\n",
    "Using any other namespace will just result in errors. First, we declare which submitter we will\n",
    "be using - we will use the ArgoSubmitter as the backend is leveraging Argo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4d988de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Argo submitter namespace: pipeline\n",
      "INFO:root:Cannot find local k8s config. Trying in-cluster config.\n",
      "INFO:root:Initialized with in-cluster config.\n"
     ]
    }
   ],
   "source": [
    "submitter = ArgoSubmitter(namespace='pipeline') # namespace must be 'pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c075eb-1f01-4aad-a418-884530665f16",
   "metadata": {},
   "source": [
    "Finally, we submit our Directed Acyclic Graph (DAG) that represents our \"pipeline\" we defined \n",
    "above to the Executor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d5dfeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Checking workflow name/generatedName runpy-\n",
      "INFO:root:Submitting workflow to Argo\n",
      "INFO:root:Workflow runpy-j8w5g has been submitted in \"pipeline\" namespace!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'argoproj.io/v1alpha1',\n",
       " 'kind': 'Workflow',\n",
       " 'metadata': {'creationTimestamp': '2022-09-27T02:44:08Z',\n",
       "  'generateName': 'runpy-',\n",
       "  'generation': 1,\n",
       "  'managedFields': [{'apiVersion': 'argoproj.io/v1alpha1',\n",
       "    'fieldsType': 'FieldsV1',\n",
       "    'fieldsV1': {'f:metadata': {'f:generateName': {}}, 'f:spec': {}},\n",
       "    'manager': 'OpenAPI-Generator',\n",
       "    'operation': 'Update',\n",
       "    'time': '2022-09-27T02:44:08Z'}],\n",
       "  'name': 'runpy-j8w5g',\n",
       "  'namespace': 'pipeline',\n",
       "  'resourceVersion': '42779483',\n",
       "  'uid': '6a864b5c-68d4-4bf0-8c2e-e4053f0b02e5'},\n",
       " 'spec': {'entrypoint': 'runpy',\n",
       "  'templates': [{'dag': {'tasks': [{'arguments': {'parameters': [{'name': 'para-A-0',\n",
       "          'value': 'A'}]},\n",
       "       'name': 'A',\n",
       "       'template': 'A'},\n",
       "      {'arguments': {'parameters': [{'name': 'para-B-0', 'value': 'B'}]},\n",
       "       'dependencies': ['A'],\n",
       "       'name': 'B',\n",
       "       'template': 'B'},\n",
       "      {'arguments': {'parameters': [{'name': 'para-C-0', 'value': 'C'}]},\n",
       "       'dependencies': ['A'],\n",
       "       'name': 'C',\n",
       "       'template': 'C'},\n",
       "      {'arguments': {'parameters': [{'name': 'para-D-0', 'value': 'D'}]},\n",
       "       'dependencies': ['B', 'C'],\n",
       "       'name': 'D',\n",
       "       'template': 'D'}]},\n",
       "    'name': 'runpy'},\n",
       "   {'container': {'args': ['{{inputs.parameters.para-A-0}}'],\n",
       "     'command': ['cowsay'],\n",
       "     'image': 'docker/whalesay:latest'},\n",
       "    'inputs': {'parameters': [{'name': 'para-A-0'}]},\n",
       "    'name': 'A',\n",
       "    'nodeSelector': {'pipeline': 'small'}},\n",
       "   {'container': {'args': ['{{inputs.parameters.para-B-0}}'],\n",
       "     'command': ['cowsay'],\n",
       "     'image': 'docker/whalesay:latest'},\n",
       "    'inputs': {'parameters': [{'name': 'para-B-0'}]},\n",
       "    'name': 'B',\n",
       "    'nodeSelector': {'pipeline': 'small'}},\n",
       "   {'container': {'args': ['{{inputs.parameters.para-C-0}}'],\n",
       "     'command': ['cowsay'],\n",
       "     'image': 'docker/whalesay:latest'},\n",
       "    'inputs': {'parameters': [{'name': 'para-C-0'}]},\n",
       "    'name': 'C',\n",
       "    'nodeSelector': {'pipeline': 'small'}},\n",
       "   {'container': {'args': ['{{inputs.parameters.para-D-0}}'],\n",
       "     'command': ['cowsay'],\n",
       "     'image': 'docker/whalesay:latest'},\n",
       "    'inputs': {'parameters': [{'name': 'para-D-0'}]},\n",
       "    'name': 'D',\n",
       "    'nodeSelector': {'pipeline': 'small'}}],\n",
       "  'tolerations': [{'effect': 'NoSchedule',\n",
       "    'key': 'ga.nodepool/type',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'}]}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment = couler.run(submitter=submitter)\n",
    "deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3fef5e",
   "metadata": {},
   "source": [
    "The following screenshot shows the successful run that the above JSON pipeline object represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3b125",
   "metadata": {},
   "source": [
    "![pipeline DAG](../images/getting_started_images/09_pipeline-dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce277570",
   "metadata": {},
   "source": [
    "And finally, the output of the above pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5de5e",
   "metadata": {},
   "source": [
    "![whalesayAB](../images/getting_started_images/09_whalesayAB.png)![whalesayCD](../images/getting_started_images/09_whalesayCD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e26167",
   "metadata": {},
   "source": [
    "These screenshots were gathered from our Pipeline UI: [https://pipeline.eo4ph.geoanalytics.ca](https://pipeline.eo4ph.geoanalytics.ca). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d481c95-8b02-4dec-bbce-3a23ba712b9a",
   "metadata": {},
   "source": [
    "## 3. Building A Pipeline that Handles Failures \n",
    "\n",
    "In this example, we create a pipeline that handles failures in previous steps gracefully. \n",
    "\n",
    "Purely to illustrate how failures can be handled, we are using [Sen2Cor](https://step.esa.int/main/snap-supported-plugins/sen2cor/) as an example. **Note: were are not actually running Sen2Cor here!**. The idea is to begin with the most recent version of Sen2Cor and then if an error occurs while\n",
    "processing the L1C input data then the pipeline will failover to the next release of Sen2Cor. \n",
    "If the process fails over on all conditions, an error job is thrown to perform \n",
    "what would be any cleanup and notification that processing of the file failed. \n",
    "\n",
    "This pipeline can be adapted into many other uses. \n",
    "\n",
    "First, after importing our libraries, we build a template job function that can take in `callable` objects to be inserted in to a Python3.6 image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d7f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import couler.argo as couler\n",
    "from couler.argo_submitter import ArgoSubmitter\n",
    "from couler.core.templates.toleration import Toleration\n",
    "from couler.core.templates.volume_claim import VolumeClaimTemplate\n",
    "from couler.core.constants import WFStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1da06e-af05-4522-b168-12c32ff18973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job(name: str, source: callable):\n",
    "    # two tolerations are needed to ensure the job is scheduled on the correct GEOAnalytics Canada cluster nodes.\n",
    "    toleration = Toleration('ga.nodepool/type', 'NoSchedule', 'Exists')\n",
    "    couler.add_toleration(toleration) # pipeline/nodepool=pipe:NoSchedule\n",
    "    toleration2 = Toleration('kubernetes.azure.com/scalesetpriority', 'NoSchedule', 'Exists')\n",
    "    couler.add_toleration(toleration2)\n",
    "    # we aren't actually going to run Sen2Cor, instead we're just running a docker image that runs python version 3.6\n",
    "    # If we were actually going to run Sen2Cor, we would have to create a docker image for each version of Sen2Cor we want to run.\n",
    "    # The arguments for run_script() is here :https://github.com/couler-proj/couler/blob/master/couler/core/run_templates.py#L41\n",
    "    return couler.run_script(\n",
    "        image=\"python:alpine3.6\",\n",
    "        # the source will be injected inside of the container and run \n",
    "        source=source,\n",
    "        step_name=name,\n",
    "        node_selector={'pipeline':'small'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be09a24",
   "metadata": {},
   "source": [
    "Next, we define our steps that represent success or failure of running the Sen2Cor binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3452ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_files():\n",
    "    return ['ras1','ras2','ras3','ras4']\n",
    "\n",
    "def preprocess():\n",
    "    print(f'preprocess')\n",
    "    \n",
    "def sen2cor290():  \n",
    "    # choose randomly if this function will succeed or fail\n",
    "    import random\n",
    "    task = ['success', 'fail']\n",
    "    res = random.randint(0, 1)\n",
    "    res = task[res]\n",
    "    print(f'{res}')\n",
    "    if res == 'fail':\n",
    "        sys.exit(2)\n",
    "\n",
    "def sen2cor280():\n",
    "    # choose randomly if this function will succeed or fail\n",
    "    import random\n",
    "    task = ['success', 'fail']\n",
    "    res = random.randint(0, 1)\n",
    "    res = task[res]\n",
    "    print(f'{res}')\n",
    "    if res == 'fail':\n",
    "        sys.exit(2)\n",
    "    \n",
    "def sen2cor255():\n",
    "    # choose randomly if this function will succeed or fail\n",
    "    import random\n",
    "    task = ['success', 'fail']\n",
    "    res = random.randint(0, 1)\n",
    "    res = task[res]\n",
    "    print(f'{res}')\n",
    "    if res == 'fail':\n",
    "        sys.exit(2)\n",
    "\n",
    "def fin():\n",
    "    print('fin')\n",
    "\n",
    "def err():\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba254f3",
   "metadata": {},
   "source": [
    "Once decalared, we wrap our functions inside of a submittable job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e776b7f8-b2fe-4722-a7cc-db3969fdbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_job():\n",
    "    return job(name='preprocess', source=preprocess)\n",
    "\n",
    "def sen2cor290_job():\n",
    "    return job(name='sen2cor290', source=sen2cor290)\n",
    "\n",
    "def sen2cor280_job():\n",
    "    return job(name='sen2cor280', source=sen2cor280)\n",
    "\n",
    "def sen2cor255_job():\n",
    "    return job(name='sen2cor255', source=sen2cor255)\n",
    "\n",
    "def fin_job():\n",
    "    return job(name='fin', source=fin)\n",
    "\n",
    "def err_job():\n",
    "    return job(name='err', source=err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d65c0",
   "metadata": {},
   "source": [
    "We now need to build our pipeline. \n",
    "\n",
    "First we gather our files, which is generally a list and then any necessary preprocessing steps. \n",
    "Once ready, the input is passed into our first Step: \"Sen2Cor version 2.9.0\". \n",
    "Using Boolean logic and the set_dependencies function, we can determine how the failovers are managed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1f0df77-e1da-4eca-bfbb-1d61db664c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_failover_pipeline():\n",
    "\n",
    "    couler.set_dependencies(\n",
    "        preprocess_job, \n",
    "        dependencies=None\n",
    "    )\n",
    "    \n",
    "    couler.set_dependencies(\n",
    "        sen2cor290_job,\n",
    "        dependencies='preprocess.Succeeded'\n",
    "    )\n",
    "\n",
    "    couler.set_dependencies(\n",
    "        sen2cor280_job,\n",
    "        dependencies='sen2cor290.Failed'\n",
    "    )\n",
    "\n",
    "    couler.set_dependencies(\n",
    "        sen2cor255_job,\n",
    "        dependencies='sen2cor280.Failed'\n",
    "    )\n",
    "    \n",
    "    couler.set_dependencies(\n",
    "        err_job,\n",
    "        dependencies='sen2cor280.Failed && sen2cor290.Failed && sen2cor255.Failed'\n",
    "    )\n",
    "    \n",
    "    couler.set_dependencies(\n",
    "        fin_job,\n",
    "        dependencies='sen2cor290.Succeeded || sen2cor280.Succeeded || sen2cor255.Succeeded'\n",
    "    )\n",
    "    \n",
    "setup_failover_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23388c",
   "metadata": {},
   "source": [
    "Finally, we submit our job to the Executor! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c917d2-6e3e-475c-99b9-c42294fb7102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Argo submitter namespace: pipeline\n",
      "INFO:root:Cannot find local k8s config. Trying in-cluster config.\n",
      "INFO:root:Initialized with in-cluster config.\n"
     ]
    }
   ],
   "source": [
    "submitter = ArgoSubmitter(namespace=os.getenv('WORKFLOW_NS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc2d6fe2-b83e-4793-a7c7-15b137fdfdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Checking workflow name/generatedName runpy-\n",
      "INFO:root:Submitting workflow to Argo\n",
      "INFO:root:Workflow runpy-knlcd has been submitted in \"pipeline\" namespace!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'argoproj.io/v1alpha1',\n",
       " 'kind': 'Workflow',\n",
       " 'metadata': {'creationTimestamp': '2022-09-27T02:37:01Z',\n",
       "  'generateName': 'runpy-',\n",
       "  'generation': 1,\n",
       "  'managedFields': [{'apiVersion': 'argoproj.io/v1alpha1',\n",
       "    'fieldsType': 'FieldsV1',\n",
       "    'fieldsV1': {'f:metadata': {'f:generateName': {}}, 'f:spec': {}},\n",
       "    'manager': 'OpenAPI-Generator',\n",
       "    'operation': 'Update',\n",
       "    'time': '2022-09-27T02:37:01Z'}],\n",
       "  'name': 'runpy-knlcd',\n",
       "  'namespace': 'pipeline',\n",
       "  'resourceVersion': '42775569',\n",
       "  'uid': 'd3fef132-6be1-47f6-bd21-0c8ef710b7d0'},\n",
       " 'spec': {'entrypoint': 'runpy',\n",
       "  'templates': [{'dag': {'tasks': [{'name': 'preprocess',\n",
       "       'template': 'preprocess'},\n",
       "      {'depends': 'preprocess.Succeeded',\n",
       "       'name': 'sen2cor290',\n",
       "       'template': 'sen2cor290'},\n",
       "      {'depends': 'sen2cor290.Failed',\n",
       "       'name': 'sen2cor280',\n",
       "       'template': 'sen2cor280'},\n",
       "      {'depends': 'sen2cor280.Failed',\n",
       "       'name': 'sen2cor255',\n",
       "       'template': 'sen2cor255'},\n",
       "      {'depends': 'sen2cor280.Failed && sen2cor290.Failed && sen2cor255.Failed',\n",
       "       'name': 'err',\n",
       "       'template': 'err'},\n",
       "      {'depends': 'sen2cor290.Succeeded || sen2cor280.Succeeded || sen2cor255.Succeeded',\n",
       "       'name': 'fin',\n",
       "       'template': 'fin'}]},\n",
       "    'name': 'runpy'},\n",
       "   {'name': 'preprocess',\n",
       "    'nodeSelector': {'pipeline': 'small'},\n",
       "    'script': {'command': ['python'],\n",
       "     'image': 'python:alpine3.6',\n",
       "     'source': \"\\nprint(f'preprocess')\\n\"}},\n",
       "   {'name': 'sen2cor290',\n",
       "    'nodeSelector': {'pipeline': 'small'},\n",
       "    'script': {'command': ['python'],\n",
       "     'image': 'python:alpine3.6',\n",
       "     'source': \"\\ntask = ['success', 'fail']\\nres = random.randint(0, 1)\\nres = task[res]\\nprint(f'{res}')\\nif res == 'fail':\\n    sys.exit(2)\\n\"}},\n",
       "   {'name': 'sen2cor280',\n",
       "    'nodeSelector': {'pipeline': 'small'},\n",
       "    'script': {'command': ['python'],\n",
       "     'image': 'python:alpine3.6',\n",
       "     'source': \"\\ntask = ['success', 'fail']\\nres = random.randint(0, 1)\\nres = task[res]\\nprint(f'{res}')\\nif res == 'fail':\\n    sys.exit(2)\\n\"}},\n",
       "   {'name': 'sen2cor255',\n",
       "    'nodeSelector': {'pipeline': 'small'},\n",
       "    'script': {'command': ['python'],\n",
       "     'image': 'python:alpine3.6',\n",
       "     'source': \"\\ntask = ['success', 'fail']\\nres = random.randint(0, 1)\\nres = task[res]\\nprint(f'{res}')\\nif res == 'fail':\\n    sys.exit(2)\\n\"}},\n",
       "   {'name': 'err',\n",
       "    'nodeSelector': {'pipeline': 'small'},\n",
       "    'script': {'command': ['python'],\n",
       "     'image': 'python:alpine3.6',\n",
       "     'source': \"\\nprint('error')\\n\"}},\n",
       "   {'name': 'fin',\n",
       "    'nodeSelector': {'pipeline': 'small'},\n",
       "    'script': {'command': ['python'],\n",
       "     'image': 'python:alpine3.6',\n",
       "     'source': \"\\nprint('fin')\\n\"}}],\n",
       "  'tolerations': [{'effect': 'NoSchedule',\n",
       "    'key': 'ga.nodepool/type',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule', 'key': 'ga.nodepool/type', 'operator': 'Exists'},\n",
       "   {'effect': 'NoSchedule',\n",
       "    'key': 'kubernetes.azure.com/scalesetpriority',\n",
       "    'operator': 'Exists'}]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "couler.run(submitter=submitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97919bd5-e16a-4337-a0f9-21f3b7081943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 10:17:43) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
