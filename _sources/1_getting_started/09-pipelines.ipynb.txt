{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dd65bc4",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "The GeoAnalytics Canada Pipeline system helps with developing and building \n",
    "portable, scalable Earth Observation pre-processing pipelines and machine \n",
    "learning (ML) workflows based on Docker containers.\n",
    "The underlying system uses Argo Workflows and the Argo Project documentation\n",
    "can be useful in debugging/expanding the pipelines functionality. \n",
    "\n",
    "**The Pipelines platform consists of:**\n",
    "\n",
    "* A UI for managing and tracking pipelines and their execution.\n",
    "* An engine for scheduling a pipelineâ€™s execution\n",
    "* An SDK for defining, building, and deploying pipelines in Python. \n",
    "* The SDK we use is the \n",
    "* [Hera python library](https://https://hera-workflows.readthedocs.io).\n",
    "\n",
    "A pipeline is a representation of a workflow containing the parameters required \n",
    "to run the workflow and the inputs and outputs of each component. \n",
    "Each pipeline component is a self-contained code block, packaged as a Docker image.\n",
    "\n",
    "A Workflow can also leverage GEOAnalytics Canada's Cloud Storage as an \n",
    "Artifact store to share artifacts between Tasks/Steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9983489-bb07-4092-9651-f4cf7e70187b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T03:43:10.981219Z",
     "iopub.status.busy": "2023-05-17T03:43:10.980345Z",
     "iopub.status.idle": "2023-05-17T03:43:10.986337Z",
     "shell.execute_reply": "2023-05-17T03:43:10.985104Z",
     "shell.execute_reply.started": "2023-05-17T03:43:10.981173Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required libraries \n",
    "import os\n",
    "\n",
    "from hera.shared import global_config\n",
    "from hera.workflows import Container, Workflow, Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "563f46a5",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Setting some global values within the notebook helps keep the workflow easier to\n",
    "update to different Container Registries, Containers, and Images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc78f93c-982b-402b-8625-5184e78e1b2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T03:43:12.077308Z",
     "iopub.status.busy": "2023-05-17T03:43:12.076845Z",
     "iopub.status.idle": "2023-05-17T03:43:12.081501Z",
     "shell.execute_reply": "2023-05-17T03:43:12.080570Z",
     "shell.execute_reply.started": "2023-05-17T03:43:12.077270Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "# -------------------------\n",
    "CR_URL=\"someregistry.domain.com\"\n",
    "IMG_LABEL=\"repository/imagename\"\n",
    "TAG_LABEL=\"0.1.0\"\n",
    "IMG_TAG=f\"{CR_URL}/{IMG_LABEL}:{TAG_LABEL}\"\n",
    "# -------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d0fa3d1",
   "metadata": {},
   "source": [
    "### Setting Up The Workflow\n",
    "\n",
    "The next Cell implements a template of how a single-step Workflow would be\n",
    "implemented. \n",
    "This Workflow is created with a single Container which is then executed \n",
    "during the Steps procedure within the Workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87b4699-54a5-4018-ad1e-e5d338c2da7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T03:43:13.242977Z",
     "iopub.status.busy": "2023-05-17T03:43:13.242118Z",
     "iopub.status.idle": "2023-05-17T03:43:13.249358Z",
     "shell.execute_reply": "2023-05-17T03:43:13.248530Z",
     "shell.execute_reply.started": "2023-05-17T03:43:13.242943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_config.api_version = \"argoproj.io/v1\"\n",
    "global_config.host = os.getenv('WORKFLOW_HOST')\n",
    "\n",
    "with Workflow(\n",
    "    name='unique-name-of-workflow', # Must be unique and lowercase\n",
    "    namespace=os.getenv('WORKFLOW_NS'), # Namespace to run workflow in <- Preconfigured in GEOAnalytics Canada\n",
    "    entrypoint='name-of-entry-task-step', # The name of the entrypoint task/step\n",
    "    parallelism=1, # Number of tasks to run in parallel\n",
    ") as w:\n",
    "    \n",
    "    # This section defines a template that can be used in multiple steps\n",
    "    t = Container(\n",
    "    name='unique-container-name',\n",
    "    image=f'{IMG_TAG}',\n",
    "    command=[\"sh\", \"./entrypoint.sh\"], # \n",
    ")\n",
    "\n",
    "    # This section defines the entrypoint task/step\n",
    "    # Here you can organise the flow of your workflow\n",
    "    with Steps(name=\"name-of-step-template\"):\n",
    "        t(name=\"name-of-task-step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102ce5c",
   "metadata": {},
   "source": [
    "> **Note**  \n",
    "> Contact your GEOAnalytics Canada Administrator for more information about available resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df5330",
   "metadata": {},
   "source": [
    "Here is an alternative approach to writing Workflows - It is recommended\n",
    "to use a context-manager to ensure objects are properly gc'd.\n",
    "This example demonstrates how to use a Python function as an input to a Task \n",
    "instead of a Docker Image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614fa669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hera.env import Env\n",
    "from hera.workflow import Workflow, Task\n",
    "\n",
    "w = Workflow(\n",
    "    name='unique-name-of-workflow',\n",
    "    namespace=os.getenv('WORKFLOW_NS'),\n",
    "    entrypoint='source-task',\n",
    "    parallelism=10, # Number of tasks to run in parallel \n",
    ")\n",
    "\n",
    "\n",
    "def some_func():\n",
    "  import os\n",
    "  #do something\n",
    "  print(os.getenv('TASKSAY'))\n",
    "\n",
    "# Environment Variables for running source/container\n",
    "env_list = [\n",
    "  Env(name='SOME_ENV', value='SOME_VAL'),\n",
    "  Env(name='TASKSAY', value='Workflows Are Powerful!')\n",
    "]\n",
    "\n",
    "# Task using a function as input\n",
    "t1 = Task(\n",
    "  name='source-task',\n",
    "  image='registry.eo4ph.geoanalytics.ca/project-name/image-name:image-tag',\n",
    "  source=some_func,\n",
    "  env=env_list\n",
    ")\n",
    "\n",
    "# Task using a prebuilt Docker Image running some Python application\n",
    "t2 = Task(\n",
    "  name='container-task',\n",
    "  image='registry.eo4ph.geoanalytics.ca/project-name/image-name:image-tag'\n",
    "  command=['/bin/bash', '-c', 'python run.py'],\n",
    "  env=env_list\n",
    ")\n",
    "\n",
    "w.add_task(t1) # Add source-task to workflow\n",
    "w.add_task(t2) # Add container-task to workflow\n",
    "t1 >> t2 # DAG - make t1 run before t2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97827a0a",
   "metadata": {},
   "source": [
    "#### Submitting A Workflow to Pipelines\n",
    "\n",
    "All that is left to do to submit your workflow, is to run the `.create()` \n",
    "method on the workflow object. \n",
    "This uses the GEOAnalytics Canada preconfigured backend settings to ensure \n",
    "your workflows are submitted with the correct permissions and security. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d15704-ef6c-4c8d-914e-0abe3c2c0223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T03:43:15.304177Z",
     "iopub.status.busy": "2023-05-17T03:43:15.303534Z",
     "iopub.status.idle": "2023-05-17T03:43:15.344254Z",
     "shell.execute_reply": "2023-05-17T03:43:15.343509Z",
     "shell.execute_reply.started": "2023-05-17T03:43:15.304147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w.create()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a270926",
   "metadata": {},
   "source": [
    "## Workflow Approaches\n",
    "\n",
    "There are different ways to organize your workflow. \n",
    "It will depend on the application/use-case and goal of your system that will\n",
    "dictate how it will end up flowing. \n",
    "You can leverage different mechanisms to control the flow/state and described below.\n",
    "\n",
    "- Steps and Parallelism\n",
    "- Artifact Passing\n",
    "- Accessing the GEOAnalytics Shared Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5b0809a",
   "metadata": {},
   "source": [
    "### Steps and Parallelism\n",
    "\n",
    "The following workflow highlights a strength of using Pipelines - parallelism. \n",
    "Concurrently executing decoupled or independent tasks is possible\n",
    "by using the `Steps.parallel()` method. \n",
    "The following workflow will first execute `A` and then run `B` and `C` at the same time. \n",
    "Without using `Steps.parallel()`, the Steps would then be executed sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21005228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following source was used to create the below code:\n",
    "# https://hera.readthedocs.io/en/latest/examples/workflows/steps_with_callable_container/\n",
    "\n",
    "import os\n",
    "\n",
    "from hera.shared import global_config\n",
    "from hera.workflows import Container, Parameter, Workflow, Steps\n",
    "\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "CR_URL=\"docker.io\"\n",
    "IMG_LABEL=\"docker/whalesay\"\n",
    "TAG_LABEL=\"latest\"\n",
    "IMG_TAG=f\"{CR_URL}/{IMG_LABEL}:{TAG_LABEL}\"\n",
    "# -------------------------\n",
    "\n",
    "global_config.api_version = \"argoproj.io/v1\"\n",
    "global_config.host = os.getenv('WORKFLOW_HOST')\n",
    "\n",
    "with Workflow(\n",
    "    name='step-parallel',\n",
    "    namespace=os.getenv('WORKFLOW_NS'),\n",
    "    entrypoint='workflowsteps',\n",
    "    parallelism=1, # Number of tasks to run in parallel\n",
    ") as w:\n",
    "    container_task = Container(\n",
    "        name='whalesay-geoanalytics',\n",
    "        image=f'{IMG_TAG}',\n",
    "        command=[\"cowsay\"],\n",
    "        inputs=[Parameter(name=\"message\")],\n",
    "        args=[\"{{inputs.parameters.message}}\"],\n",
    "    )\n",
    "\n",
    "    with Steps(name='workflowsteps') as s:\n",
    "        container_task(\n",
    "            name='A', \n",
    "            arguments=[Parameter(name='message', value='Hi!')]\n",
    "        )\n",
    "\n",
    "        with s.parallel():\n",
    "            container_task(\n",
    "                name='B',\n",
    "                arguments=[Parameter(name='message', value='Hello!')]\n",
    "            )\n",
    "            container_task(\n",
    "                name='C',\n",
    "                arguments=[Parameter(name='message', value='General Kenobi!')]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5e6ad",
   "metadata": {},
   "source": [
    "The above Workflow would result in a DAG that looks like this:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Task A]\n",
    "    B[Task B]\n",
    "    C[Task C]\n",
    "    D[Finalised]\n",
    "    A --> B & C --> D\n",
    "```\n",
    "\n",
    "A preceedes B and C, which are run in parallel. \n",
    "An extra task, D, is included for verbosity and to show that the workflow is complete."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb68b8e8",
   "metadata": {},
   "source": [
    "### Artifact Passing \n",
    "\n",
    "It a lot of cases it's necessary to pass an output to the next Step of\n",
    "a workflow. \n",
    "Artifacts solve this scenario. \n",
    "You can create an Artifact as an output from a previous Step to \n",
    "be consumed by a subsequent step. \n",
    "The Workflow below demonstrates an Artifact being created and then \n",
    "consumed in the next Step. \n",
    "An important thing to notice is that the output and input paths \n",
    "differ - this is to allow more flexible insertion of information in\n",
    "your Step implementations. \n",
    "\n",
    "> **Note**  \n",
    "> Using our `shared-data` Blob Container as the Artifact Repository \n",
    "> allows  artifacts to be archived in an accessible \n",
    "> location for all users on the tenant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from hera.shared import global_config\n",
    "from hera.workflows import Container, Artifact, Workflow, Steps, Step\n",
    "\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "IMG_LABEL=\"busybox\"\n",
    "TAG_LABEL=\"1.36.0\"\n",
    "IMG_TAG=f\"{IMG_LABEL}:{TAG_LABEL}\"\n",
    "# -------------------------\n",
    "\n",
    "global_config.api_version = \"argoproj.io/v1\"\n",
    "global_config.host = os.getenv('WORKFLOW_HOST')\n",
    "\n",
    "with Workflow(\n",
    "    name='artifact-passing',\n",
    "    namespace=os.getenv('WORKFLOW_NS'),\n",
    "    entrypoint='workflowsteps',\n",
    "    parallelism=1, # Number of tasks to run in parallel\n",
    ") as w:\n",
    "    create_file = Container(\n",
    "        name='creator',\n",
    "        image=f'{IMG_TAG}',\n",
    "        command=['/bin/sh', '-c', 'echo \"Hello from Task1\" >> /tmp/hello.txt'],\n",
    "        outputs=[Artifact(name='hellomessage', path='/tmp/hello.txt')]\n",
    "    )\n",
    "    read_file = Container(\n",
    "        name='reader',\n",
    "        image=f'{IMG_TAG}',\n",
    "        command=[\"cat\"],\n",
    "        args=[\"/tmp/artifact/hello.txt\"],\n",
    "        inputs=[Artifact(name='inputmessage', path='/tmp/artifact/hello.txt')]\n",
    "    )\n",
    "    with Steps(name='workflowsteps') as s:\n",
    "        Step(name='createfile', template=create_file)\n",
    "        Step(\n",
    "            name='readfile',\n",
    "            template=read_file,\n",
    "            arguments=[Artifact(\n",
    "                name='inputmessage', \n",
    "                from_='{{steps.createfile.outputs.artifacts.hellomessage}}',\n",
    "                subpath='/tmp/hello.txt'\n",
    "            )]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ec4ae",
   "metadata": {},
   "source": [
    "The above Workflow will generate a DAG that looks like this:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Create File]\n",
    "    artifactStore[(artifact-store)]\n",
    "    B[Read File]\n",
    "    A --'write'--o artifactStore\n",
    "    artifactStore o--'read'--o B\n",
    "    A --> B\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bee5912",
   "metadata": {},
   "source": [
    "### Accessing the Shared Data Bucket\n",
    "\n",
    "We can access the `shared-data` bucket found in GEOAnalytics Canada \n",
    "that is used as a common data storage location for each tenant group. \n",
    "\n",
    "Using a pre-populated file `read-test.txt` that contains the string\n",
    "`\"Hello from the Bucket!\"`, we can `cat` the file contents to stdout\n",
    "and observe that in the UI logs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from hera.shared import global_config\n",
    "from hera.workflows import Container, Workflow, Steps\n",
    "from hera.workflows.models import VolumeMount\n",
    "\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "IMG_LABEL=\"busybox\"\n",
    "TAG_LABEL=\"1.36.0\"\n",
    "IMG_TAG=f\"{IMG_LABEL}:{TAG_LABEL}\"\n",
    "# -------------------------\n",
    "\n",
    "global_config.api_version = \"argoproj.io/v1\"\n",
    "global_config.host = os.getenv('WORKFLOW_HOST')\n",
    "\n",
    "with Workflow(\n",
    "    name='shared-data-access',\n",
    "    namespace=os.getenv('WORKFLOW_NS'),\n",
    "    entrypoint='workflowsteps',\n",
    "    parallelism=1, # Number of tasks to run in parallel\n",
    ") as w:\n",
    "    container_task = Container(\n",
    "        name='busybox-geoanalytics',\n",
    "        image=f'{IMG_TAG}',\n",
    "        command=[\"cat\", \"/mnt/vol/shared-data/test-read.txt\"],\n",
    "        volume_mounts=[\n",
    "            VolumeMount(name=\"shared-data\", mount_path=\"/mnt/vol\"),\n",
    "        ],\n",
    "    )\n",
    "    with Steps(name='workflowsteps') as s:\n",
    "        container_task(\n",
    "            name='readit'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0034d8",
   "metadata": {},
   "source": [
    "The above Workflow is similar in concept to the Artifact store except that the Cloud storage is directly mounted to the \n",
    "Workflow Task Container for it to access via the filesystem. This is useful for large datasets that are too large to\n",
    "upload or manage with the Artifact store.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    SharedData[(shared-data)]\n",
    "    A[readit Task]\n",
    "    B[output]\n",
    "    SharedData --'mount'--> A\n",
    "    A --'read file'--> A\n",
    "    A --> B\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2ebe937",
   "metadata": {},
   "source": [
    "## Inspect Workflow\n",
    "\n",
    "You can inspect portions of your workflow by leveraging the \n",
    "`IPython.display.JSON` widget. \n",
    "Some parameters may be missing - there are some defaults that are \n",
    "intserted by the Workflow Controller that enable the workflow to execute\n",
    "in the GEOAnalytics Platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d009558-3074-40d3-b275-571a5204cd83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T03:43:45.258730Z",
     "iopub.status.busy": "2023-05-17T03:43:45.257953Z",
     "iopub.status.idle": "2023-05-17T03:43:45.264296Z",
     "shell.execute_reply": "2023-05-17T03:43:45.263587Z",
     "shell.execute_reply.started": "2023-05-17T03:43:45.258700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "JSON(w.to_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "154484ee",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Argo Workflows](https://argoproj.github.io/argo-workflows/) - The Pipelines underlying tool: Argo Workflows\n",
    "- [hera API Reference](https://hera-workflows.readthedocs.io/en/latest/api/shared/) - API documentation for `hera` library\n",
    "- [hera GitHub](https://github.com/argoproj-labs/hera) - Source code for `hera`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c59aad6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
